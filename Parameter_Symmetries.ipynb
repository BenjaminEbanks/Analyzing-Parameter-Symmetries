{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOSBj+e5DTNl3hNEj7uLuL5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjaminEbanks/Analyzing-Parameter-Symmetries/blob/main/Parameter_Symmetries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utils**"
      ],
      "metadata": {
        "id": "BROii2Vxyc2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def flatten_params(model):\n",
        "  return model.state_dict()\n",
        "\n",
        "def lerp(lam, t1, t2):\n",
        "  t3 = copy.deepcopy(t2)\n",
        "  for p in t1:\n",
        "    t3[p] = (1 - lam) * t1[p] + lam * t2[p]\n",
        "  return t3"
      ],
      "metadata": {
        "id": "c8uZJBV_xwgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_interp_acc(lambdas, train_acc_interp_naive, test_acc_interp_naive,\n",
        "                    train_acc_interp_clever, test_acc_interp_clever):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.plot(lambdas,\n",
        "          train_acc_interp_naive,\n",
        "          linestyle=\"dashed\",\n",
        "          color=\"tab:blue\",\n",
        "          alpha=0.5,\n",
        "          linewidth=2,\n",
        "          label=\"Train, naïve interp.\")\n",
        "  ax.plot(lambdas,\n",
        "          test_acc_interp_naive,\n",
        "          linestyle=\"dashed\",\n",
        "          color=\"tab:orange\",\n",
        "          alpha=0.5,\n",
        "          linewidth=2,\n",
        "          label=\"Test, naïve interp.\")\n",
        "  ax.plot(lambdas,\n",
        "          train_acc_interp_clever,\n",
        "          linestyle=\"solid\",\n",
        "          color=\"tab:blue\",\n",
        "          linewidth=2,\n",
        "          label=\"Train, permuted interp.\")\n",
        "  ax.plot(lambdas,\n",
        "          test_acc_interp_clever,\n",
        "          linestyle=\"solid\",\n",
        "          color=\"tab:orange\",\n",
        "          linewidth=2,\n",
        "          label=\"Test, permuted interp.\")\n",
        "  ax.set_xlabel(\"$\\lambda$\")\n",
        "  ax.set_xticks([0, 1])\n",
        "  ax.set_xticklabels([\"Model $A$\", \"Model $B$\"])\n",
        "  ax.set_ylabel(\"Accuracy\")\n",
        "  # TODO label x=0 tick as \\theta_1, and x=1 tick as \\theta_2\n",
        "  ax.set_title(f\"Accuracy between the two models\")\n",
        "  ax.legend(loc=\"lower right\", framealpha=0.5)\n",
        "  fig.tight_layout()\n",
        "  return fig"
      ],
      "metadata": {
        "id": "fg7OpDIw9BMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from re import L\n",
        "from typing import NamedTuple\n",
        "import torch\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "\n",
        "class PermutationSpec(NamedTuple):\n",
        "  perm_to_axes: dict\n",
        "  axes_to_perm: dict\n",
        "\n",
        "def permutation_spec_from_axes_to_perm(axes_to_perm: dict) -> PermutationSpec:\n",
        "  perm_to_axes = defaultdict(list)\n",
        "  for wk, axis_perms in axes_to_perm.items():\n",
        "    for axis, perm in enumerate(axis_perms):\n",
        "      if perm is not None:\n",
        "        perm_to_axes[perm].append((wk, axis))\n",
        "  return PermutationSpec(perm_to_axes=dict(perm_to_axes), axes_to_perm=axes_to_perm)\n",
        "\n",
        "def mlp_permutation_spec(num_hidden_layers: int) -> PermutationSpec:\n",
        "  \"\"\"We assume that one permutation cannot appear in two axes of the same weight array.\"\"\"\n",
        "  assert num_hidden_layers >= 1\n",
        "  return permutation_spec_from_axes_to_perm({\n",
        "      \"layer0.weight\": (\"P_0\", None),\n",
        "      **{f\"layer{i}.weight\": ( f\"P_{i}\", f\"P_{i-1}\")\n",
        "         for i in range(1, num_hidden_layers)},\n",
        "      **{f\"layer{i}.bias\": (f\"P_{i}\", )\n",
        "         for i in range(num_hidden_layers)},\n",
        "      f\"layer{num_hidden_layers}.weight\": (None, f\"P_{num_hidden_layers-1}\"),\n",
        "      f\"layer{num_hidden_layers}.bias\": (None, ),\n",
        "  })\n",
        "\n",
        "\"\"\"\n",
        "def cnn_permutation_spec() -> PermutationSpec:\n",
        "  conv = lambda name, p_in, p_out: {f\"{name}.weight\": (p_out, p_in, None, None, )}\n",
        "  dense = lambda name, p_in, p_out, bias=True: {f\"{name}.weight\": (p_out, p_in), f\"{name}.bias\": (p_out, )} if bias else  {f\"{name}.weight\": (p_out, p_in)}\n",
        "\n",
        "  return permutation_spec_from_axes_to_perm({\n",
        "     **conv(\"conv1\", None, \"P_bg0\"),\n",
        "     **conv(\"conv2\", \"P_bg0\", \"P_bg1\"),\n",
        "     **dense(\"fc1\", \"P_bg1\", \"P_bg2\"),\n",
        "     **dense(\"fc2\", \"P_bg2\", None, False),\n",
        "  })\n",
        "\"\"\"\n",
        "\n",
        "def get_permuted_param(ps: PermutationSpec, perm, k: str, params, except_axis=None):\n",
        "  \"\"\"Get parameter `k` from `params`, with the permutations applied.\"\"\"\n",
        "  w = params[k]\n",
        "  for axis, p in enumerate(ps.axes_to_perm[k]):\n",
        "    # Skip the axis we're trying to permute.\n",
        "    if axis == except_axis:\n",
        "      continue\n",
        "\n",
        "    # None indicates that there is no permutation relevant to that axis.\n",
        "    if p is not None:\n",
        "        w = torch.index_select(w, axis, perm[p].int())\n",
        "\n",
        "  return w\n",
        "\n",
        "def apply_permutation(ps: PermutationSpec, perm, params):\n",
        "  \"\"\"Apply a `perm` to `params`.\"\"\"\n",
        "  return {k: get_permuted_param(ps, perm, k, params) for k in params.keys()}\n",
        "\n",
        "def weight_matching(ps: PermutationSpec, params_a, params_b, max_iter=100, init_perm=None):\n",
        "  \"\"\"Find a permutation of `params_b` to make them match `params_a`.\"\"\"\n",
        "  perm_sizes = {p: params_a[axes[0][0]].shape[axes[0][1]] for p, axes in ps.perm_to_axes.items()}\n",
        "\n",
        "  perm = {p: torch.arange(n) for p, n in perm_sizes.items()} if init_perm is None else init_perm\n",
        "  perm_names = list(perm.keys())\n",
        "\n",
        "  for iteration in range(max_iter):\n",
        "    progress = False\n",
        "    for p_ix in torch.randperm(len(perm_names)):\n",
        "      p = perm_names[p_ix]\n",
        "      n = perm_sizes[p]\n",
        "      A = torch.zeros((n, n))\n",
        "      for wk, axis in ps.perm_to_axes[p]:\n",
        "        w_a = params_a[wk]\n",
        "        w_b = get_permuted_param(ps, perm, wk, params_b, except_axis=axis)\n",
        "        w_a = torch.moveaxis(w_a, axis, 0).reshape((n, -1))\n",
        "        w_b = torch.moveaxis(w_b, axis, 0).reshape((n, -1))\n",
        "\n",
        "        A += w_a @ w_b.T\n",
        "\n",
        "      ri, ci = linear_sum_assignment(A.detach().numpy(), maximize=True)\n",
        "      assert (torch.tensor(ri) == torch.arange(len(ri))).all()\n",
        "      oldL = torch.einsum('ij,ij->i', A, torch.eye(n)[perm[p].long()]).sum()\n",
        "      newL = torch.einsum('ij,ij->i', A,torch.eye(n)[ci, :]).sum()\n",
        "      print(f\"{iteration}/{p}: {newL - oldL}\")\n",
        "      progress = progress or newL > oldL + 1e-12\n",
        "\n",
        "      perm[p] = torch.Tensor(ci)\n",
        "\n",
        "    if not progress:\n",
        "      break\n",
        "\n",
        "  return perm\n",
        "\n",
        "# def test_weight_matching():\n",
        "#   \"\"\"If we just have a single hidden layer then it should converge after just one step.\"\"\"\n",
        "#   ps = mlp_permutation_spec(num_hidden_layers=3)\n",
        "#   print(ps.axes_to_perm)\n",
        "#   rng = torch.Generator()\n",
        "#   rng.manual_seed(13)\n",
        "#   num_hidden = 10\n",
        "#   shapes = {\n",
        "#       \"layer0.weight\": (2, num_hidden),\n",
        "#       \"layer0.bias\": (num_hidden, ),\n",
        "#       \"layer1.weight\": (num_hidden, 3),\n",
        "#       \"layer1.bias\": (3, )\n",
        "#   }\n",
        "\n",
        "#   params_a = {k: torch.randn(shape, generator=rng) for k, shape in shapes.items()}\n",
        "#   params_b = {k: torch.randn(shape, generator=rng) for k, shape in shapes.items()}\n",
        "#   perm = weight_matching(ps, params_a, params_b)  # Removed rng from the arguments\n",
        "#   print(perm)\n",
        "\n",
        "# test_weight_matching()"
      ],
      "metadata": {
        "id": "4-zpjBgb0mJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch, softmax=False):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        if softmax:\n",
        "            output = F.log_softmax(output, dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            acc = 100. * correct / len(train_loader.dataset)\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    acc = 100. * correct / len(train_loader.dataset)\n",
        "    print('Train Accuracy: ({:.0f}%) '.format(acc))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, softmax=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            if softmax:\n",
        "                output = F.log_softmax(output, dim=1)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    print('\\nAverage loss: {:.4f}, Accuracy: ({:.0f}%)\\n'.format(\n",
        "        test_loss, acc))\n",
        "    return test_loss, acc"
      ],
      "metadata": {
        "id": "g9z0I2dVyVmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0vhvf97xhqg"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input=28*28):\n",
        "\n",
        "    super().__init__()\n",
        "    self.input = input\n",
        "    self.layer0 = nn.Linear(input, 512)\n",
        "    self.layer1 = nn.Linear(512, 512)\n",
        "    self.layer2 = nn.Linear(512, 512)\n",
        "    self.layer3 = nn.Linear(512, 256)\n",
        "    self.layer4 = nn.Linear(256, 10)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, self.input)\n",
        "    x = nn.functional.relu(self.layer0(x))\n",
        "    x = nn.functional.relu(self.layer1(x))\n",
        "    x = nn.functional.relu(self.layer2(x))\n",
        "    x = nn.functional.relu(self.layer3(x))\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    return nn.functional.log_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self, seed, batch_size=512, epochs=50, lr=0.001, log_interval=10):\n",
        "        self.seed = seed\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.log_interval = log_interval\n",
        "\n",
        "args_1 = Args(seed=1)\n",
        "args_2 = Args(seed=2)"
      ],
      "metadata": {
        "id": "CzgIpJe5iskv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# def main():\n",
        "  # print('hello')\n",
        "  # parser = argparse.ArgumentParser()\n",
        "  # parser.add_argument('--seed', type=int, default=1)\n",
        "  # parser.add_argument('--batch_size', type=int, default=512)\n",
        "  # parser.add_argument('--epochs', type=int, default=50)\n",
        "  # parser.add_argument(\"--lr\", type=float, required=True)\n",
        "  # parser.add_argument('--log-interval', type=int, default=10, metavar='N',help='how many batches to wait before logging training status')\n",
        "  # args = parser.parse_args()\n",
        "\n",
        "  # # Get data\n",
        "  # print(args)\n",
        "  # args = parser.parse_args()\n",
        "\n",
        "# class Args:\n",
        "#   seed = 1\n",
        "#   batch_size = 512\n",
        "#   epochs = 50\n",
        "#   lr = 0.001  # Set your desired learning rate here\n",
        "#   log_interval = 10\n",
        "\n",
        "# args_2 = Args()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args_2.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {'batch_size': args_2.batch_size}\n",
        "test_kwargs = {'batch_size': args_2.batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "model_b = MLP().to(device)\n",
        "optimizer = optim.Adam(model_b.parameters(), lr=args_2.lr)\n",
        "\n",
        "for epoch in range(1, args_2.epochs + 1):\n",
        "    train(args_2, model_b, device, train_loader, optimizer, epoch)\n",
        "    test(model_b, device, test_loader)\n",
        "\n",
        "torch.save(model_b.state_dict(), f\"mnist_mlp_{str(args_2.seed)}.pt\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4RFP3cCUSgM",
        "outputId": "2533894a-e3e0-4149-f6c9-ea545f229542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-3775b565d8d0>:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return nn.functional.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304119\n",
            "Train Epoch: 1 [5120/60000 (8%)]\tLoss: 0.861866\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.464231\n",
            "Train Epoch: 1 [15360/60000 (25%)]\tLoss: 0.412749\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.406430\n",
            "Train Epoch: 1 [25600/60000 (42%)]\tLoss: 0.234064\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.278593\n",
            "Train Epoch: 1 [35840/60000 (59%)]\tLoss: 0.243973\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.301197\n",
            "Train Epoch: 1 [46080/60000 (76%)]\tLoss: 0.244700\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.138244\n",
            "Train Epoch: 1 [56320/60000 (93%)]\tLoss: 0.147130\n",
            "Train Accuracy: (87%) \n",
            "\n",
            "Average loss: 0.1929, Accuracy: (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.184348\n",
            "Train Epoch: 2 [5120/60000 (8%)]\tLoss: 0.148041\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.135490\n",
            "Train Epoch: 2 [15360/60000 (25%)]\tLoss: 0.162000\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.212680\n",
            "Train Epoch: 2 [25600/60000 (42%)]\tLoss: 0.114324\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.125971\n",
            "Train Epoch: 2 [35840/60000 (59%)]\tLoss: 0.124478\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.171876\n",
            "Train Epoch: 2 [46080/60000 (76%)]\tLoss: 0.143732\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.083708\n",
            "Train Epoch: 2 [56320/60000 (93%)]\tLoss: 0.075713\n",
            "Train Accuracy: (96%) \n",
            "\n",
            "Average loss: 0.1072, Accuracy: (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.117367\n",
            "Train Epoch: 3 [5120/60000 (8%)]\tLoss: 0.088752\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.063999\n",
            "Train Epoch: 3 [15360/60000 (25%)]\tLoss: 0.097317\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.135725\n",
            "Train Epoch: 3 [25600/60000 (42%)]\tLoss: 0.063495\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.103460\n",
            "Train Epoch: 3 [35840/60000 (59%)]\tLoss: 0.077055\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.102800\n",
            "Train Epoch: 3 [46080/60000 (76%)]\tLoss: 0.099466\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.060651\n",
            "Train Epoch: 3 [56320/60000 (93%)]\tLoss: 0.061687\n",
            "Train Accuracy: (97%) \n",
            "\n",
            "Average loss: 0.0970, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.103884\n",
            "Train Epoch: 4 [5120/60000 (8%)]\tLoss: 0.066694\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.042319\n",
            "Train Epoch: 4 [15360/60000 (25%)]\tLoss: 0.085295\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.100367\n",
            "Train Epoch: 4 [25600/60000 (42%)]\tLoss: 0.043943\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.101969\n",
            "Train Epoch: 4 [35840/60000 (59%)]\tLoss: 0.059090\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.101251\n",
            "Train Epoch: 4 [46080/60000 (76%)]\tLoss: 0.075515\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.047534\n",
            "Train Epoch: 4 [56320/60000 (93%)]\tLoss: 0.057377\n",
            "Train Accuracy: (98%) \n",
            "\n",
            "Average loss: 0.0925, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.093143\n",
            "Train Epoch: 5 [5120/60000 (8%)]\tLoss: 0.044690\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.062110\n",
            "Train Epoch: 5 [15360/60000 (25%)]\tLoss: 0.052699\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.057843\n",
            "Train Epoch: 5 [25600/60000 (42%)]\tLoss: 0.045234\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.059706\n",
            "Train Epoch: 5 [35840/60000 (59%)]\tLoss: 0.055218\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.065731\n",
            "Train Epoch: 5 [46080/60000 (76%)]\tLoss: 0.064426\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.064207\n",
            "Train Epoch: 5 [56320/60000 (93%)]\tLoss: 0.040679\n",
            "Train Accuracy: (98%) \n",
            "\n",
            "Average loss: 0.1034, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.072571\n",
            "Train Epoch: 6 [5120/60000 (8%)]\tLoss: 0.037923\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.025794\n",
            "Train Epoch: 6 [15360/60000 (25%)]\tLoss: 0.028387\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.032933\n",
            "Train Epoch: 6 [25600/60000 (42%)]\tLoss: 0.040164\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.030585\n",
            "Train Epoch: 6 [35840/60000 (59%)]\tLoss: 0.029312\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.069377\n",
            "Train Epoch: 6 [46080/60000 (76%)]\tLoss: 0.043252\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.045742\n",
            "Train Epoch: 6 [56320/60000 (93%)]\tLoss: 0.026361\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.1007, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.073042\n",
            "Train Epoch: 7 [5120/60000 (8%)]\tLoss: 0.032050\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.016249\n",
            "Train Epoch: 7 [15360/60000 (25%)]\tLoss: 0.060530\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.039643\n",
            "Train Epoch: 7 [25600/60000 (42%)]\tLoss: 0.034597\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.055487\n",
            "Train Epoch: 7 [35840/60000 (59%)]\tLoss: 0.049179\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.048492\n",
            "Train Epoch: 7 [46080/60000 (76%)]\tLoss: 0.033235\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.046746\n",
            "Train Epoch: 7 [56320/60000 (93%)]\tLoss: 0.017202\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.1242, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.101511\n",
            "Train Epoch: 8 [5120/60000 (8%)]\tLoss: 0.020736\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.033923\n",
            "Train Epoch: 8 [15360/60000 (25%)]\tLoss: 0.009806\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.024946\n",
            "Train Epoch: 8 [25600/60000 (42%)]\tLoss: 0.017618\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.015888\n",
            "Train Epoch: 8 [35840/60000 (59%)]\tLoss: 0.064919\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.042466\n",
            "Train Epoch: 8 [46080/60000 (76%)]\tLoss: 0.013560\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.028251\n",
            "Train Epoch: 8 [56320/60000 (93%)]\tLoss: 0.012438\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0959, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.033358\n",
            "Train Epoch: 9 [5120/60000 (8%)]\tLoss: 0.025391\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.010265\n",
            "Train Epoch: 9 [15360/60000 (25%)]\tLoss: 0.014439\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.020767\n",
            "Train Epoch: 9 [25600/60000 (42%)]\tLoss: 0.021763\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.024185\n",
            "Train Epoch: 9 [35840/60000 (59%)]\tLoss: 0.012194\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.054338\n",
            "Train Epoch: 9 [46080/60000 (76%)]\tLoss: 0.023664\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.019611\n",
            "Train Epoch: 9 [56320/60000 (93%)]\tLoss: 0.016543\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0870, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.031943\n",
            "Train Epoch: 10 [5120/60000 (8%)]\tLoss: 0.009152\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.015418\n",
            "Train Epoch: 10 [15360/60000 (25%)]\tLoss: 0.017465\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.044645\n",
            "Train Epoch: 10 [25600/60000 (42%)]\tLoss: 0.034210\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.023222\n",
            "Train Epoch: 10 [35840/60000 (59%)]\tLoss: 0.027177\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.024694\n",
            "Train Epoch: 10 [46080/60000 (76%)]\tLoss: 0.037601\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.048124\n",
            "Train Epoch: 10 [56320/60000 (93%)]\tLoss: 0.020127\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.1001, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.055722\n",
            "Train Epoch: 11 [5120/60000 (8%)]\tLoss: 0.031060\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.016997\n",
            "Train Epoch: 11 [15360/60000 (25%)]\tLoss: 0.020230\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.010943\n",
            "Train Epoch: 11 [25600/60000 (42%)]\tLoss: 0.028917\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.023551\n",
            "Train Epoch: 11 [35840/60000 (59%)]\tLoss: 0.023617\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.012533\n",
            "Train Epoch: 11 [46080/60000 (76%)]\tLoss: 0.010464\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.018570\n",
            "Train Epoch: 11 [56320/60000 (93%)]\tLoss: 0.010147\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0987, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.025926\n",
            "Train Epoch: 12 [5120/60000 (8%)]\tLoss: 0.019017\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.010386\n",
            "Train Epoch: 12 [15360/60000 (25%)]\tLoss: 0.005788\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.007760\n",
            "Train Epoch: 12 [25600/60000 (42%)]\tLoss: 0.004065\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.010454\n",
            "Train Epoch: 12 [35840/60000 (59%)]\tLoss: 0.016745\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.041013\n",
            "Train Epoch: 12 [46080/60000 (76%)]\tLoss: 0.028810\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.016747\n",
            "Train Epoch: 12 [56320/60000 (93%)]\tLoss: 0.028091\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0774, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.021098\n",
            "Train Epoch: 13 [5120/60000 (8%)]\tLoss: 0.010490\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.008832\n",
            "Train Epoch: 13 [15360/60000 (25%)]\tLoss: 0.007140\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.008095\n",
            "Train Epoch: 13 [25600/60000 (42%)]\tLoss: 0.003299\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.021537\n",
            "Train Epoch: 13 [35840/60000 (59%)]\tLoss: 0.007517\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.005418\n",
            "Train Epoch: 13 [46080/60000 (76%)]\tLoss: 0.008190\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.011776\n",
            "Train Epoch: 13 [56320/60000 (93%)]\tLoss: 0.014660\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1112, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.017514\n",
            "Train Epoch: 14 [5120/60000 (8%)]\tLoss: 0.011033\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.002724\n",
            "Train Epoch: 14 [15360/60000 (25%)]\tLoss: 0.017144\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.010669\n",
            "Train Epoch: 14 [25600/60000 (42%)]\tLoss: 0.012887\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.006495\n",
            "Train Epoch: 14 [35840/60000 (59%)]\tLoss: 0.019469\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.010197\n",
            "Train Epoch: 14 [46080/60000 (76%)]\tLoss: 0.009833\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.020044\n",
            "Train Epoch: 14 [56320/60000 (93%)]\tLoss: 0.010673\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0938, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.025565\n",
            "Train Epoch: 15 [5120/60000 (8%)]\tLoss: 0.008481\n",
            "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.014418\n",
            "Train Epoch: 15 [15360/60000 (25%)]\tLoss: 0.003116\n",
            "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.039591\n",
            "Train Epoch: 15 [25600/60000 (42%)]\tLoss: 0.009483\n",
            "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.006544\n",
            "Train Epoch: 15 [35840/60000 (59%)]\tLoss: 0.007629\n",
            "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.006460\n",
            "Train Epoch: 15 [46080/60000 (76%)]\tLoss: 0.027759\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.014369\n",
            "Train Epoch: 15 [56320/60000 (93%)]\tLoss: 0.011492\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1131, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.034555\n",
            "Train Epoch: 16 [5120/60000 (8%)]\tLoss: 0.007904\n",
            "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.003150\n",
            "Train Epoch: 16 [15360/60000 (25%)]\tLoss: 0.022553\n",
            "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.022077\n",
            "Train Epoch: 16 [25600/60000 (42%)]\tLoss: 0.020213\n",
            "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.030353\n",
            "Train Epoch: 16 [35840/60000 (59%)]\tLoss: 0.021983\n",
            "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.005687\n",
            "Train Epoch: 16 [46080/60000 (76%)]\tLoss: 0.003751\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.007693\n",
            "Train Epoch: 16 [56320/60000 (93%)]\tLoss: 0.006924\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1197, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.039092\n",
            "Train Epoch: 17 [5120/60000 (8%)]\tLoss: 0.004989\n",
            "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.002757\n",
            "Train Epoch: 17 [15360/60000 (25%)]\tLoss: 0.016944\n",
            "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.018492\n",
            "Train Epoch: 17 [25600/60000 (42%)]\tLoss: 0.005705\n",
            "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.006507\n",
            "Train Epoch: 17 [35840/60000 (59%)]\tLoss: 0.029461\n",
            "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.018648\n",
            "Train Epoch: 17 [46080/60000 (76%)]\tLoss: 0.013410\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.021638\n",
            "Train Epoch: 17 [56320/60000 (93%)]\tLoss: 0.016203\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1144, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.013696\n",
            "Train Epoch: 18 [5120/60000 (8%)]\tLoss: 0.010669\n",
            "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.009341\n",
            "Train Epoch: 18 [15360/60000 (25%)]\tLoss: 0.012986\n",
            "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.001498\n",
            "Train Epoch: 18 [25600/60000 (42%)]\tLoss: 0.006439\n",
            "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.001783\n",
            "Train Epoch: 18 [35840/60000 (59%)]\tLoss: 0.000867\n",
            "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.004263\n",
            "Train Epoch: 18 [46080/60000 (76%)]\tLoss: 0.019388\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.016739\n",
            "Train Epoch: 18 [56320/60000 (93%)]\tLoss: 0.022057\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.2080, Accuracy: (96%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.077153\n",
            "Train Epoch: 19 [5120/60000 (8%)]\tLoss: 0.008182\n",
            "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.014703\n",
            "Train Epoch: 19 [15360/60000 (25%)]\tLoss: 0.008434\n",
            "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.015631\n",
            "Train Epoch: 19 [25600/60000 (42%)]\tLoss: 0.020179\n",
            "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.001656\n",
            "Train Epoch: 19 [35840/60000 (59%)]\tLoss: 0.012645\n",
            "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.017010\n",
            "Train Epoch: 19 [46080/60000 (76%)]\tLoss: 0.022359\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.031937\n",
            "Train Epoch: 19 [56320/60000 (93%)]\tLoss: 0.016748\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1050, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.014938\n",
            "Train Epoch: 20 [5120/60000 (8%)]\tLoss: 0.006411\n",
            "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.005184\n",
            "Train Epoch: 20 [15360/60000 (25%)]\tLoss: 0.028936\n",
            "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.005795\n",
            "Train Epoch: 20 [25600/60000 (42%)]\tLoss: 0.006115\n",
            "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.030191\n",
            "Train Epoch: 20 [35840/60000 (59%)]\tLoss: 0.014761\n",
            "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.029623\n",
            "Train Epoch: 20 [46080/60000 (76%)]\tLoss: 0.022188\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.012711\n",
            "Train Epoch: 20 [56320/60000 (93%)]\tLoss: 0.010696\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1198, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.006121\n",
            "Train Epoch: 21 [5120/60000 (8%)]\tLoss: 0.021243\n",
            "Train Epoch: 21 [10240/60000 (17%)]\tLoss: 0.015427\n",
            "Train Epoch: 21 [15360/60000 (25%)]\tLoss: 0.002319\n",
            "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 0.001575\n",
            "Train Epoch: 21 [25600/60000 (42%)]\tLoss: 0.008709\n",
            "Train Epoch: 21 [30720/60000 (51%)]\tLoss: 0.012166\n",
            "Train Epoch: 21 [35840/60000 (59%)]\tLoss: 0.004313\n",
            "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 0.022210\n",
            "Train Epoch: 21 [46080/60000 (76%)]\tLoss: 0.000699\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.023183\n",
            "Train Epoch: 21 [56320/60000 (93%)]\tLoss: 0.027845\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0958, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.008902\n",
            "Train Epoch: 22 [5120/60000 (8%)]\tLoss: 0.009043\n",
            "Train Epoch: 22 [10240/60000 (17%)]\tLoss: 0.027471\n",
            "Train Epoch: 22 [15360/60000 (25%)]\tLoss: 0.007496\n",
            "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 0.023687\n",
            "Train Epoch: 22 [25600/60000 (42%)]\tLoss: 0.007850\n",
            "Train Epoch: 22 [30720/60000 (51%)]\tLoss: 0.008674\n",
            "Train Epoch: 22 [35840/60000 (59%)]\tLoss: 0.027203\n",
            "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 0.017305\n",
            "Train Epoch: 22 [46080/60000 (76%)]\tLoss: 0.019487\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.025447\n",
            "Train Epoch: 22 [56320/60000 (93%)]\tLoss: 0.004440\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0830, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.007753\n",
            "Train Epoch: 23 [5120/60000 (8%)]\tLoss: 0.002027\n",
            "Train Epoch: 23 [10240/60000 (17%)]\tLoss: 0.005253\n",
            "Train Epoch: 23 [15360/60000 (25%)]\tLoss: 0.000924\n",
            "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 0.008041\n",
            "Train Epoch: 23 [25600/60000 (42%)]\tLoss: 0.000968\n",
            "Train Epoch: 23 [30720/60000 (51%)]\tLoss: 0.005472\n",
            "Train Epoch: 23 [35840/60000 (59%)]\tLoss: 0.007057\n",
            "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 0.011649\n",
            "Train Epoch: 23 [46080/60000 (76%)]\tLoss: 0.006515\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.005126\n",
            "Train Epoch: 23 [56320/60000 (93%)]\tLoss: 0.011804\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0977, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.017158\n",
            "Train Epoch: 24 [5120/60000 (8%)]\tLoss: 0.005570\n",
            "Train Epoch: 24 [10240/60000 (17%)]\tLoss: 0.005398\n",
            "Train Epoch: 24 [15360/60000 (25%)]\tLoss: 0.006347\n",
            "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 0.000838\n",
            "Train Epoch: 24 [25600/60000 (42%)]\tLoss: 0.001127\n",
            "Train Epoch: 24 [30720/60000 (51%)]\tLoss: 0.005020\n",
            "Train Epoch: 24 [35840/60000 (59%)]\tLoss: 0.015817\n",
            "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 0.004295\n",
            "Train Epoch: 24 [46080/60000 (76%)]\tLoss: 0.006747\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.008196\n",
            "Train Epoch: 24 [56320/60000 (93%)]\tLoss: 0.000533\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0988, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.001966\n",
            "Train Epoch: 25 [5120/60000 (8%)]\tLoss: 0.011650\n",
            "Train Epoch: 25 [10240/60000 (17%)]\tLoss: 0.003529\n",
            "Train Epoch: 25 [15360/60000 (25%)]\tLoss: 0.029414\n",
            "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 0.010057\n",
            "Train Epoch: 25 [25600/60000 (42%)]\tLoss: 0.014068\n",
            "Train Epoch: 25 [30720/60000 (51%)]\tLoss: 0.003885\n",
            "Train Epoch: 25 [35840/60000 (59%)]\tLoss: 0.038309\n",
            "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 0.007739\n",
            "Train Epoch: 25 [46080/60000 (76%)]\tLoss: 0.004150\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.001591\n",
            "Train Epoch: 25 [56320/60000 (93%)]\tLoss: 0.017317\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0776, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.001712\n",
            "Train Epoch: 26 [5120/60000 (8%)]\tLoss: 0.007802\n",
            "Train Epoch: 26 [10240/60000 (17%)]\tLoss: 0.004826\n",
            "Train Epoch: 26 [15360/60000 (25%)]\tLoss: 0.003833\n",
            "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 0.028395\n",
            "Train Epoch: 26 [25600/60000 (42%)]\tLoss: 0.016554\n",
            "Train Epoch: 26 [30720/60000 (51%)]\tLoss: 0.001046\n",
            "Train Epoch: 26 [35840/60000 (59%)]\tLoss: 0.020893\n",
            "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 0.007409\n",
            "Train Epoch: 26 [46080/60000 (76%)]\tLoss: 0.010248\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.023577\n",
            "Train Epoch: 26 [56320/60000 (93%)]\tLoss: 0.019320\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0917, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.001658\n",
            "Train Epoch: 27 [5120/60000 (8%)]\tLoss: 0.002294\n",
            "Train Epoch: 27 [10240/60000 (17%)]\tLoss: 0.001710\n",
            "Train Epoch: 27 [15360/60000 (25%)]\tLoss: 0.001451\n",
            "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 0.008882\n",
            "Train Epoch: 27 [25600/60000 (42%)]\tLoss: 0.002314\n",
            "Train Epoch: 27 [30720/60000 (51%)]\tLoss: 0.000911\n",
            "Train Epoch: 27 [35840/60000 (59%)]\tLoss: 0.000457\n",
            "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 0.001766\n",
            "Train Epoch: 27 [46080/60000 (76%)]\tLoss: 0.005132\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.032722\n",
            "Train Epoch: 27 [56320/60000 (93%)]\tLoss: 0.004699\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1090, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.009574\n",
            "Train Epoch: 28 [5120/60000 (8%)]\tLoss: 0.023876\n",
            "Train Epoch: 28 [10240/60000 (17%)]\tLoss: 0.003691\n",
            "Train Epoch: 28 [15360/60000 (25%)]\tLoss: 0.006941\n",
            "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 0.004803\n",
            "Train Epoch: 28 [25600/60000 (42%)]\tLoss: 0.021480\n",
            "Train Epoch: 28 [30720/60000 (51%)]\tLoss: 0.003896\n",
            "Train Epoch: 28 [35840/60000 (59%)]\tLoss: 0.003842\n",
            "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 0.001979\n",
            "Train Epoch: 28 [46080/60000 (76%)]\tLoss: 0.006098\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.003519\n",
            "Train Epoch: 28 [56320/60000 (93%)]\tLoss: 0.003990\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1049, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.000371\n",
            "Train Epoch: 29 [5120/60000 (8%)]\tLoss: 0.005072\n",
            "Train Epoch: 29 [10240/60000 (17%)]\tLoss: 0.000575\n",
            "Train Epoch: 29 [15360/60000 (25%)]\tLoss: 0.005546\n",
            "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 0.005190\n",
            "Train Epoch: 29 [25600/60000 (42%)]\tLoss: 0.008615\n",
            "Train Epoch: 29 [30720/60000 (51%)]\tLoss: 0.012373\n",
            "Train Epoch: 29 [35840/60000 (59%)]\tLoss: 0.032958\n",
            "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 0.013805\n",
            "Train Epoch: 29 [46080/60000 (76%)]\tLoss: 0.003624\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.006945\n",
            "Train Epoch: 29 [56320/60000 (93%)]\tLoss: 0.003223\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0973, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.018275\n",
            "Train Epoch: 30 [5120/60000 (8%)]\tLoss: 0.001809\n",
            "Train Epoch: 30 [10240/60000 (17%)]\tLoss: 0.001529\n",
            "Train Epoch: 30 [15360/60000 (25%)]\tLoss: 0.003995\n",
            "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 0.017494\n",
            "Train Epoch: 30 [25600/60000 (42%)]\tLoss: 0.003084\n",
            "Train Epoch: 30 [30720/60000 (51%)]\tLoss: 0.010885\n",
            "Train Epoch: 30 [35840/60000 (59%)]\tLoss: 0.003709\n",
            "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 0.014686\n",
            "Train Epoch: 30 [46080/60000 (76%)]\tLoss: 0.000720\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.003225\n",
            "Train Epoch: 30 [56320/60000 (93%)]\tLoss: 0.004707\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1068, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.009968\n",
            "Train Epoch: 31 [5120/60000 (8%)]\tLoss: 0.002319\n",
            "Train Epoch: 31 [10240/60000 (17%)]\tLoss: 0.003761\n",
            "Train Epoch: 31 [15360/60000 (25%)]\tLoss: 0.007489\n",
            "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 0.001004\n",
            "Train Epoch: 31 [25600/60000 (42%)]\tLoss: 0.006195\n",
            "Train Epoch: 31 [30720/60000 (51%)]\tLoss: 0.001086\n",
            "Train Epoch: 31 [35840/60000 (59%)]\tLoss: 0.003936\n",
            "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 0.018031\n",
            "Train Epoch: 31 [46080/60000 (76%)]\tLoss: 0.027978\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.011182\n",
            "Train Epoch: 31 [56320/60000 (93%)]\tLoss: 0.003043\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1164, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.009657\n",
            "Train Epoch: 32 [5120/60000 (8%)]\tLoss: 0.003919\n",
            "Train Epoch: 32 [10240/60000 (17%)]\tLoss: 0.001649\n",
            "Train Epoch: 32 [15360/60000 (25%)]\tLoss: 0.000507\n",
            "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 0.001553\n",
            "Train Epoch: 32 [25600/60000 (42%)]\tLoss: 0.000693\n",
            "Train Epoch: 32 [30720/60000 (51%)]\tLoss: 0.003447\n",
            "Train Epoch: 32 [35840/60000 (59%)]\tLoss: 0.005792\n",
            "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 0.027241\n",
            "Train Epoch: 32 [46080/60000 (76%)]\tLoss: 0.003380\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.016850\n",
            "Train Epoch: 32 [56320/60000 (93%)]\tLoss: 0.000748\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0800, Accuracy: (99%)\n",
            "\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.002029\n",
            "Train Epoch: 33 [5120/60000 (8%)]\tLoss: 0.005160\n",
            "Train Epoch: 33 [10240/60000 (17%)]\tLoss: 0.003187\n",
            "Train Epoch: 33 [15360/60000 (25%)]\tLoss: 0.000439\n",
            "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 0.014722\n",
            "Train Epoch: 33 [25600/60000 (42%)]\tLoss: 0.000675\n",
            "Train Epoch: 33 [30720/60000 (51%)]\tLoss: 0.003677\n",
            "Train Epoch: 33 [35840/60000 (59%)]\tLoss: 0.002415\n",
            "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 0.000937\n",
            "Train Epoch: 33 [46080/60000 (76%)]\tLoss: 0.000664\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.031831\n",
            "Train Epoch: 33 [56320/60000 (93%)]\tLoss: 0.013731\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0843, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.002981\n",
            "Train Epoch: 34 [5120/60000 (8%)]\tLoss: 0.011979\n",
            "Train Epoch: 34 [10240/60000 (17%)]\tLoss: 0.001105\n",
            "Train Epoch: 34 [15360/60000 (25%)]\tLoss: 0.010651\n",
            "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 0.001522\n",
            "Train Epoch: 34 [25600/60000 (42%)]\tLoss: 0.001697\n",
            "Train Epoch: 34 [30720/60000 (51%)]\tLoss: 0.008468\n",
            "Train Epoch: 34 [35840/60000 (59%)]\tLoss: 0.008760\n",
            "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 0.019916\n",
            "Train Epoch: 34 [46080/60000 (76%)]\tLoss: 0.004999\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.000889\n",
            "Train Epoch: 34 [56320/60000 (93%)]\tLoss: 0.000729\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1023, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.002868\n",
            "Train Epoch: 35 [5120/60000 (8%)]\tLoss: 0.012202\n",
            "Train Epoch: 35 [10240/60000 (17%)]\tLoss: 0.007461\n",
            "Train Epoch: 35 [15360/60000 (25%)]\tLoss: 0.006323\n",
            "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 0.000489\n",
            "Train Epoch: 35 [25600/60000 (42%)]\tLoss: 0.007049\n",
            "Train Epoch: 35 [30720/60000 (51%)]\tLoss: 0.000253\n",
            "Train Epoch: 35 [35840/60000 (59%)]\tLoss: 0.004289\n",
            "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 0.000286\n",
            "Train Epoch: 35 [46080/60000 (76%)]\tLoss: 0.008309\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.001140\n",
            "Train Epoch: 35 [56320/60000 (93%)]\tLoss: 0.011214\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1049, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.003029\n",
            "Train Epoch: 36 [5120/60000 (8%)]\tLoss: 0.003460\n",
            "Train Epoch: 36 [10240/60000 (17%)]\tLoss: 0.000495\n",
            "Train Epoch: 36 [15360/60000 (25%)]\tLoss: 0.004433\n",
            "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 0.005613\n",
            "Train Epoch: 36 [25600/60000 (42%)]\tLoss: 0.002218\n",
            "Train Epoch: 36 [30720/60000 (51%)]\tLoss: 0.000426\n",
            "Train Epoch: 36 [35840/60000 (59%)]\tLoss: 0.004838\n",
            "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 0.001954\n",
            "Train Epoch: 36 [46080/60000 (76%)]\tLoss: 0.000792\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.002940\n",
            "Train Epoch: 36 [56320/60000 (93%)]\tLoss: 0.002397\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1058, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.008939\n",
            "Train Epoch: 37 [5120/60000 (8%)]\tLoss: 0.001544\n",
            "Train Epoch: 37 [10240/60000 (17%)]\tLoss: 0.015146\n",
            "Train Epoch: 37 [15360/60000 (25%)]\tLoss: 0.052323\n",
            "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 0.024590\n",
            "Train Epoch: 37 [25600/60000 (42%)]\tLoss: 0.007148\n",
            "Train Epoch: 37 [30720/60000 (51%)]\tLoss: 0.001385\n",
            "Train Epoch: 37 [35840/60000 (59%)]\tLoss: 0.011010\n",
            "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 0.012131\n",
            "Train Epoch: 37 [46080/60000 (76%)]\tLoss: 0.020314\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.000838\n",
            "Train Epoch: 37 [56320/60000 (93%)]\tLoss: 0.002487\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1150, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.023476\n",
            "Train Epoch: 38 [5120/60000 (8%)]\tLoss: 0.003328\n",
            "Train Epoch: 38 [10240/60000 (17%)]\tLoss: 0.005281\n",
            "Train Epoch: 38 [15360/60000 (25%)]\tLoss: 0.002627\n",
            "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 0.012109\n",
            "Train Epoch: 38 [25600/60000 (42%)]\tLoss: 0.003214\n",
            "Train Epoch: 38 [30720/60000 (51%)]\tLoss: 0.000504\n",
            "Train Epoch: 38 [35840/60000 (59%)]\tLoss: 0.002170\n",
            "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 0.002899\n",
            "Train Epoch: 38 [46080/60000 (76%)]\tLoss: 0.012173\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.001352\n",
            "Train Epoch: 38 [56320/60000 (93%)]\tLoss: 0.000215\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0961, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.000830\n",
            "Train Epoch: 39 [5120/60000 (8%)]\tLoss: 0.007208\n",
            "Train Epoch: 39 [10240/60000 (17%)]\tLoss: 0.000260\n",
            "Train Epoch: 39 [15360/60000 (25%)]\tLoss: 0.004902\n",
            "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 0.003568\n",
            "Train Epoch: 39 [25600/60000 (42%)]\tLoss: 0.000244\n",
            "Train Epoch: 39 [30720/60000 (51%)]\tLoss: 0.006190\n",
            "Train Epoch: 39 [35840/60000 (59%)]\tLoss: 0.001277\n",
            "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 0.000524\n",
            "Train Epoch: 39 [46080/60000 (76%)]\tLoss: 0.012247\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.003552\n",
            "Train Epoch: 39 [56320/60000 (93%)]\tLoss: 0.012104\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1033, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.000584\n",
            "Train Epoch: 40 [5120/60000 (8%)]\tLoss: 0.001231\n",
            "Train Epoch: 40 [10240/60000 (17%)]\tLoss: 0.006896\n",
            "Train Epoch: 40 [15360/60000 (25%)]\tLoss: 0.000680\n",
            "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 0.005908\n",
            "Train Epoch: 40 [25600/60000 (42%)]\tLoss: 0.011377\n",
            "Train Epoch: 40 [30720/60000 (51%)]\tLoss: 0.002017\n",
            "Train Epoch: 40 [35840/60000 (59%)]\tLoss: 0.004728\n",
            "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 0.008513\n",
            "Train Epoch: 40 [46080/60000 (76%)]\tLoss: 0.001292\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.018693\n",
            "Train Epoch: 40 [56320/60000 (93%)]\tLoss: 0.001952\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0874, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.015169\n",
            "Train Epoch: 41 [5120/60000 (8%)]\tLoss: 0.001967\n",
            "Train Epoch: 41 [10240/60000 (17%)]\tLoss: 0.000746\n",
            "Train Epoch: 41 [15360/60000 (25%)]\tLoss: 0.006003\n",
            "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 0.009342\n",
            "Train Epoch: 41 [25600/60000 (42%)]\tLoss: 0.018789\n",
            "Train Epoch: 41 [30720/60000 (51%)]\tLoss: 0.004757\n",
            "Train Epoch: 41 [35840/60000 (59%)]\tLoss: 0.003160\n",
            "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 0.004947\n",
            "Train Epoch: 41 [46080/60000 (76%)]\tLoss: 0.005288\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.003157\n",
            "Train Epoch: 41 [56320/60000 (93%)]\tLoss: 0.014891\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1127, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.010187\n",
            "Train Epoch: 42 [5120/60000 (8%)]\tLoss: 0.001688\n",
            "Train Epoch: 42 [10240/60000 (17%)]\tLoss: 0.004858\n",
            "Train Epoch: 42 [15360/60000 (25%)]\tLoss: 0.004854\n",
            "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 0.009366\n",
            "Train Epoch: 42 [25600/60000 (42%)]\tLoss: 0.000234\n",
            "Train Epoch: 42 [30720/60000 (51%)]\tLoss: 0.022492\n",
            "Train Epoch: 42 [35840/60000 (59%)]\tLoss: 0.016206\n",
            "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 0.004187\n",
            "Train Epoch: 42 [46080/60000 (76%)]\tLoss: 0.020204\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.001855\n",
            "Train Epoch: 42 [56320/60000 (93%)]\tLoss: 0.013848\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1154, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.005229\n",
            "Train Epoch: 43 [5120/60000 (8%)]\tLoss: 0.009393\n",
            "Train Epoch: 43 [10240/60000 (17%)]\tLoss: 0.006694\n",
            "Train Epoch: 43 [15360/60000 (25%)]\tLoss: 0.000432\n",
            "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 0.002162\n",
            "Train Epoch: 43 [25600/60000 (42%)]\tLoss: 0.001937\n",
            "Train Epoch: 43 [30720/60000 (51%)]\tLoss: 0.010407\n",
            "Train Epoch: 43 [35840/60000 (59%)]\tLoss: 0.002381\n",
            "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 0.004312\n",
            "Train Epoch: 43 [46080/60000 (76%)]\tLoss: 0.005425\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.000562\n",
            "Train Epoch: 43 [56320/60000 (93%)]\tLoss: 0.002325\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1171, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.009346\n",
            "Train Epoch: 44 [5120/60000 (8%)]\tLoss: 0.008600\n",
            "Train Epoch: 44 [10240/60000 (17%)]\tLoss: 0.000871\n",
            "Train Epoch: 44 [15360/60000 (25%)]\tLoss: 0.012577\n",
            "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 0.010015\n",
            "Train Epoch: 44 [25600/60000 (42%)]\tLoss: 0.003848\n",
            "Train Epoch: 44 [30720/60000 (51%)]\tLoss: 0.002229\n",
            "Train Epoch: 44 [35840/60000 (59%)]\tLoss: 0.003613\n",
            "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 0.019136\n",
            "Train Epoch: 44 [46080/60000 (76%)]\tLoss: 0.005445\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.000942\n",
            "Train Epoch: 44 [56320/60000 (93%)]\tLoss: 0.001880\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1045, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.002174\n",
            "Train Epoch: 45 [5120/60000 (8%)]\tLoss: 0.000992\n",
            "Train Epoch: 45 [10240/60000 (17%)]\tLoss: 0.000674\n",
            "Train Epoch: 45 [15360/60000 (25%)]\tLoss: 0.007060\n",
            "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 0.022282\n",
            "Train Epoch: 45 [25600/60000 (42%)]\tLoss: 0.003941\n",
            "Train Epoch: 45 [30720/60000 (51%)]\tLoss: 0.011348\n",
            "Train Epoch: 45 [35840/60000 (59%)]\tLoss: 0.002964\n",
            "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 0.008814\n",
            "Train Epoch: 45 [46080/60000 (76%)]\tLoss: 0.001679\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.000880\n",
            "Train Epoch: 45 [56320/60000 (93%)]\tLoss: 0.000694\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0767, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.002009\n",
            "Train Epoch: 46 [5120/60000 (8%)]\tLoss: 0.002194\n",
            "Train Epoch: 46 [10240/60000 (17%)]\tLoss: 0.001227\n",
            "Train Epoch: 46 [15360/60000 (25%)]\tLoss: 0.001584\n",
            "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 0.002910\n",
            "Train Epoch: 46 [25600/60000 (42%)]\tLoss: 0.003494\n",
            "Train Epoch: 46 [30720/60000 (51%)]\tLoss: 0.005969\n",
            "Train Epoch: 46 [35840/60000 (59%)]\tLoss: 0.001750\n",
            "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 0.001473\n",
            "Train Epoch: 46 [46080/60000 (76%)]\tLoss: 0.002267\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.000592\n",
            "Train Epoch: 46 [56320/60000 (93%)]\tLoss: 0.001693\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0965, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.000781\n",
            "Train Epoch: 47 [5120/60000 (8%)]\tLoss: 0.005765\n",
            "Train Epoch: 47 [10240/60000 (17%)]\tLoss: 0.000668\n",
            "Train Epoch: 47 [15360/60000 (25%)]\tLoss: 0.002714\n",
            "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 0.004427\n",
            "Train Epoch: 47 [25600/60000 (42%)]\tLoss: 0.000631\n",
            "Train Epoch: 47 [30720/60000 (51%)]\tLoss: 0.001169\n",
            "Train Epoch: 47 [35840/60000 (59%)]\tLoss: 0.006597\n",
            "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 0.001398\n",
            "Train Epoch: 47 [46080/60000 (76%)]\tLoss: 0.012886\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.010706\n",
            "Train Epoch: 47 [56320/60000 (93%)]\tLoss: 0.001381\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0766, Accuracy: (99%)\n",
            "\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.000712\n",
            "Train Epoch: 48 [5120/60000 (8%)]\tLoss: 0.000136\n",
            "Train Epoch: 48 [10240/60000 (17%)]\tLoss: 0.000408\n",
            "Train Epoch: 48 [15360/60000 (25%)]\tLoss: 0.000154\n",
            "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 0.000074\n",
            "Train Epoch: 48 [25600/60000 (42%)]\tLoss: 0.000396\n",
            "Train Epoch: 48 [30720/60000 (51%)]\tLoss: 0.003097\n",
            "Train Epoch: 48 [35840/60000 (59%)]\tLoss: 0.011375\n",
            "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 0.001377\n",
            "Train Epoch: 48 [46080/60000 (76%)]\tLoss: 0.000260\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.003200\n",
            "Train Epoch: 48 [56320/60000 (93%)]\tLoss: 0.000692\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0810, Accuracy: (99%)\n",
            "\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.000493\n",
            "Train Epoch: 49 [5120/60000 (8%)]\tLoss: 0.001011\n",
            "Train Epoch: 49 [10240/60000 (17%)]\tLoss: 0.004162\n",
            "Train Epoch: 49 [15360/60000 (25%)]\tLoss: 0.000971\n",
            "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 0.000364\n",
            "Train Epoch: 49 [25600/60000 (42%)]\tLoss: 0.001255\n",
            "Train Epoch: 49 [30720/60000 (51%)]\tLoss: 0.000269\n",
            "Train Epoch: 49 [35840/60000 (59%)]\tLoss: 0.002337\n",
            "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 0.000860\n",
            "Train Epoch: 49 [46080/60000 (76%)]\tLoss: 0.000480\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.000122\n",
            "Train Epoch: 49 [56320/60000 (93%)]\tLoss: 0.003160\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0881, Accuracy: (99%)\n",
            "\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.001812\n",
            "Train Epoch: 50 [5120/60000 (8%)]\tLoss: 0.001000\n",
            "Train Epoch: 50 [10240/60000 (17%)]\tLoss: 0.000602\n",
            "Train Epoch: 50 [15360/60000 (25%)]\tLoss: 0.008619\n",
            "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 0.000058\n",
            "Train Epoch: 50 [25600/60000 (42%)]\tLoss: 0.000699\n",
            "Train Epoch: 50 [30720/60000 (51%)]\tLoss: 0.000191\n",
            "Train Epoch: 50 [35840/60000 (59%)]\tLoss: 0.000301\n",
            "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 0.003406\n",
            "Train Epoch: 50 [46080/60000 (76%)]\tLoss: 0.001005\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.001755\n",
            "Train Epoch: 50 [56320/60000 (93%)]\tLoss: 0.000877\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0935, Accuracy: (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# def main():\n",
        "  # print('hello')\n",
        "  # parser = argparse.ArgumentParser()\n",
        "  # parser.add_argument('--seed', type=int, default=1)\n",
        "  # parser.add_argument('--batch_size', type=int, default=512)\n",
        "  # parser.add_argument('--epochs', type=int, default=50)\n",
        "  # parser.add_argument(\"--lr\", type=float, required=True)\n",
        "  # parser.add_argument('--log-interval', type=int, default=10, metavar='N',help='how many batches to wait before logging training status')\n",
        "  # args = parser.parse_args()\n",
        "\n",
        "  # # Get data\n",
        "  # print(args)\n",
        "  # args = parser.parse_args()\n",
        "\n",
        "# class Args:\n",
        "#   seed = 1\n",
        "#   batch_size = 512\n",
        "#   epochs = 50\n",
        "#   lr = 0.001  # Set your desired learning rate here\n",
        "#   log_interval = 10\n",
        "\n",
        "# args_1 = Args()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args_1.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {'batch_size': args_1.batch_size}\n",
        "test_kwargs = {'batch_size': args_1.batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "model_a = MLP().to(device)\n",
        "optimizer = optim.Adam(model_a.parameters(), lr=args_1.lr)\n",
        "\n",
        "for epoch in range(1, args_1.epochs + 1):\n",
        "    train(args_1, model_a, device, train_loader, optimizer, epoch)\n",
        "    test(model_a, device, test_loader)\n",
        "\n",
        "torch.save(model_a.state_dict(), f\"mnist_mlp_{str(args_1.seed)}.pt\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   main()"
      ],
      "metadata": {
        "id": "kDO0Y10yyPjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feae1102-2611-41d8-94d8-7d2d79596869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-3775b565d8d0>:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return nn.functional.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301238\n",
            "Train Epoch: 1 [5120/60000 (8%)]\tLoss: 0.837421\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.370069\n",
            "Train Epoch: 1 [15360/60000 (25%)]\tLoss: 0.437956\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.448421\n",
            "Train Epoch: 1 [25600/60000 (42%)]\tLoss: 0.255471\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.292597\n",
            "Train Epoch: 1 [35840/60000 (59%)]\tLoss: 0.252196\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.268243\n",
            "Train Epoch: 1 [46080/60000 (76%)]\tLoss: 0.249866\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.150546\n",
            "Train Epoch: 1 [56320/60000 (93%)]\tLoss: 0.156378\n",
            "Train Accuracy: (87%) \n",
            "\n",
            "Average loss: 0.1810, Accuracy: (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.168576\n",
            "Train Epoch: 2 [5120/60000 (8%)]\tLoss: 0.143115\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.129787\n",
            "Train Epoch: 2 [15360/60000 (25%)]\tLoss: 0.157175\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.205519\n",
            "Train Epoch: 2 [25600/60000 (42%)]\tLoss: 0.120471\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.130757\n",
            "Train Epoch: 2 [35840/60000 (59%)]\tLoss: 0.141258\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.177783\n",
            "Train Epoch: 2 [46080/60000 (76%)]\tLoss: 0.149881\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.091875\n",
            "Train Epoch: 2 [56320/60000 (93%)]\tLoss: 0.083609\n",
            "Train Accuracy: (96%) \n",
            "\n",
            "Average loss: 0.1156, Accuracy: (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.107616\n",
            "Train Epoch: 3 [5120/60000 (8%)]\tLoss: 0.093603\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.055365\n",
            "Train Epoch: 3 [15360/60000 (25%)]\tLoss: 0.095866\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.143008\n",
            "Train Epoch: 3 [25600/60000 (42%)]\tLoss: 0.080038\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.108784\n",
            "Train Epoch: 3 [35840/60000 (59%)]\tLoss: 0.083263\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.097535\n",
            "Train Epoch: 3 [46080/60000 (76%)]\tLoss: 0.095182\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.067802\n",
            "Train Epoch: 3 [56320/60000 (93%)]\tLoss: 0.072822\n",
            "Train Accuracy: (97%) \n",
            "\n",
            "Average loss: 0.1084, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.103742\n",
            "Train Epoch: 4 [5120/60000 (8%)]\tLoss: 0.058172\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.052648\n",
            "Train Epoch: 4 [15360/60000 (25%)]\tLoss: 0.078443\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.099137\n",
            "Train Epoch: 4 [25600/60000 (42%)]\tLoss: 0.065376\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.104245\n",
            "Train Epoch: 4 [35840/60000 (59%)]\tLoss: 0.066459\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.084389\n",
            "Train Epoch: 4 [46080/60000 (76%)]\tLoss: 0.078456\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.042866\n",
            "Train Epoch: 4 [56320/60000 (93%)]\tLoss: 0.050805\n",
            "Train Accuracy: (98%) \n",
            "\n",
            "Average loss: 0.1004, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.089043\n",
            "Train Epoch: 5 [5120/60000 (8%)]\tLoss: 0.056198\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.025608\n",
            "Train Epoch: 5 [15360/60000 (25%)]\tLoss: 0.047144\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.041716\n",
            "Train Epoch: 5 [25600/60000 (42%)]\tLoss: 0.060095\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.068383\n",
            "Train Epoch: 5 [35840/60000 (59%)]\tLoss: 0.035096\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.085493\n",
            "Train Epoch: 5 [46080/60000 (76%)]\tLoss: 0.048912\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.054172\n",
            "Train Epoch: 5 [56320/60000 (93%)]\tLoss: 0.063509\n",
            "Train Accuracy: (98%) \n",
            "\n",
            "Average loss: 0.1125, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.069850\n",
            "Train Epoch: 6 [5120/60000 (8%)]\tLoss: 0.038732\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.040144\n",
            "Train Epoch: 6 [15360/60000 (25%)]\tLoss: 0.031522\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.045304\n",
            "Train Epoch: 6 [25600/60000 (42%)]\tLoss: 0.024728\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.055840\n",
            "Train Epoch: 6 [35840/60000 (59%)]\tLoss: 0.035257\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.066448\n",
            "Train Epoch: 6 [46080/60000 (76%)]\tLoss: 0.037798\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.063738\n",
            "Train Epoch: 6 [56320/60000 (93%)]\tLoss: 0.024177\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.1446, Accuracy: (96%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.067504\n",
            "Train Epoch: 7 [5120/60000 (8%)]\tLoss: 0.035175\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.009605\n",
            "Train Epoch: 7 [15360/60000 (25%)]\tLoss: 0.054376\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.051084\n",
            "Train Epoch: 7 [25600/60000 (42%)]\tLoss: 0.034546\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.041355\n",
            "Train Epoch: 7 [35840/60000 (59%)]\tLoss: 0.044939\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.075246\n",
            "Train Epoch: 7 [46080/60000 (76%)]\tLoss: 0.041837\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.026263\n",
            "Train Epoch: 7 [56320/60000 (93%)]\tLoss: 0.022391\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0841, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.054090\n",
            "Train Epoch: 8 [5120/60000 (8%)]\tLoss: 0.022918\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.015210\n",
            "Train Epoch: 8 [15360/60000 (25%)]\tLoss: 0.049221\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.046686\n",
            "Train Epoch: 8 [25600/60000 (42%)]\tLoss: 0.032124\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.039682\n",
            "Train Epoch: 8 [35840/60000 (59%)]\tLoss: 0.031896\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.069106\n",
            "Train Epoch: 8 [46080/60000 (76%)]\tLoss: 0.021093\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.019236\n",
            "Train Epoch: 8 [56320/60000 (93%)]\tLoss: 0.018811\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.1032, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.061264\n",
            "Train Epoch: 9 [5120/60000 (8%)]\tLoss: 0.039297\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.030592\n",
            "Train Epoch: 9 [15360/60000 (25%)]\tLoss: 0.032707\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.023360\n",
            "Train Epoch: 9 [25600/60000 (42%)]\tLoss: 0.026276\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.026711\n",
            "Train Epoch: 9 [35840/60000 (59%)]\tLoss: 0.034779\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.026722\n",
            "Train Epoch: 9 [46080/60000 (76%)]\tLoss: 0.038776\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.032208\n",
            "Train Epoch: 9 [56320/60000 (93%)]\tLoss: 0.016559\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0932, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.037133\n",
            "Train Epoch: 10 [5120/60000 (8%)]\tLoss: 0.012575\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.005625\n",
            "Train Epoch: 10 [15360/60000 (25%)]\tLoss: 0.005676\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.013566\n",
            "Train Epoch: 10 [25600/60000 (42%)]\tLoss: 0.005313\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.015403\n",
            "Train Epoch: 10 [35840/60000 (59%)]\tLoss: 0.033682\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.037601\n",
            "Train Epoch: 10 [46080/60000 (76%)]\tLoss: 0.038731\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.014321\n",
            "Train Epoch: 10 [56320/60000 (93%)]\tLoss: 0.018091\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0942, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.034531\n",
            "Train Epoch: 11 [5120/60000 (8%)]\tLoss: 0.012171\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.023385\n",
            "Train Epoch: 11 [15360/60000 (25%)]\tLoss: 0.011232\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.007533\n",
            "Train Epoch: 11 [25600/60000 (42%)]\tLoss: 0.014813\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.014787\n",
            "Train Epoch: 11 [35840/60000 (59%)]\tLoss: 0.019302\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.015967\n",
            "Train Epoch: 11 [46080/60000 (76%)]\tLoss: 0.037307\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.025439\n",
            "Train Epoch: 11 [56320/60000 (93%)]\tLoss: 0.029560\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.1126, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.031500\n",
            "Train Epoch: 12 [5120/60000 (8%)]\tLoss: 0.019599\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.022911\n",
            "Train Epoch: 12 [15360/60000 (25%)]\tLoss: 0.012996\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.006202\n",
            "Train Epoch: 12 [25600/60000 (42%)]\tLoss: 0.006348\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.021045\n",
            "Train Epoch: 12 [35840/60000 (59%)]\tLoss: 0.011833\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.021525\n",
            "Train Epoch: 12 [46080/60000 (76%)]\tLoss: 0.050103\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.031798\n",
            "Train Epoch: 12 [56320/60000 (93%)]\tLoss: 0.031022\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.1338, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.034596\n",
            "Train Epoch: 13 [5120/60000 (8%)]\tLoss: 0.012812\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.007675\n",
            "Train Epoch: 13 [15360/60000 (25%)]\tLoss: 0.005358\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.010109\n",
            "Train Epoch: 13 [25600/60000 (42%)]\tLoss: 0.005877\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.027425\n",
            "Train Epoch: 13 [35840/60000 (59%)]\tLoss: 0.049752\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.018315\n",
            "Train Epoch: 13 [46080/60000 (76%)]\tLoss: 0.016073\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.007650\n",
            "Train Epoch: 13 [56320/60000 (93%)]\tLoss: 0.011952\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0962, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.016734\n",
            "Train Epoch: 14 [5120/60000 (8%)]\tLoss: 0.007038\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.002741\n",
            "Train Epoch: 14 [15360/60000 (25%)]\tLoss: 0.024935\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.014704\n",
            "Train Epoch: 14 [25600/60000 (42%)]\tLoss: 0.017157\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.003155\n",
            "Train Epoch: 14 [35840/60000 (59%)]\tLoss: 0.009654\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.012902\n",
            "Train Epoch: 14 [46080/60000 (76%)]\tLoss: 0.008783\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.055074\n",
            "Train Epoch: 14 [56320/60000 (93%)]\tLoss: 0.017770\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1182, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.019615\n",
            "Train Epoch: 15 [5120/60000 (8%)]\tLoss: 0.022930\n",
            "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.025230\n",
            "Train Epoch: 15 [15360/60000 (25%)]\tLoss: 0.012689\n",
            "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.004037\n",
            "Train Epoch: 15 [25600/60000 (42%)]\tLoss: 0.017916\n",
            "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.022978\n",
            "Train Epoch: 15 [35840/60000 (59%)]\tLoss: 0.014544\n",
            "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.025139\n",
            "Train Epoch: 15 [46080/60000 (76%)]\tLoss: 0.019152\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.040581\n",
            "Train Epoch: 15 [56320/60000 (93%)]\tLoss: 0.025896\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1230, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.034746\n",
            "Train Epoch: 16 [5120/60000 (8%)]\tLoss: 0.025517\n",
            "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.014107\n",
            "Train Epoch: 16 [15360/60000 (25%)]\tLoss: 0.014983\n",
            "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.010840\n",
            "Train Epoch: 16 [25600/60000 (42%)]\tLoss: 0.002255\n",
            "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.016137\n",
            "Train Epoch: 16 [35840/60000 (59%)]\tLoss: 0.008413\n",
            "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.025955\n",
            "Train Epoch: 16 [46080/60000 (76%)]\tLoss: 0.023661\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.008132\n",
            "Train Epoch: 16 [56320/60000 (93%)]\tLoss: 0.001031\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1072, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.014591\n",
            "Train Epoch: 17 [5120/60000 (8%)]\tLoss: 0.012712\n",
            "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.008355\n",
            "Train Epoch: 17 [15360/60000 (25%)]\tLoss: 0.022899\n",
            "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.025843\n",
            "Train Epoch: 17 [25600/60000 (42%)]\tLoss: 0.003689\n",
            "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.003358\n",
            "Train Epoch: 17 [35840/60000 (59%)]\tLoss: 0.018976\n",
            "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.006634\n",
            "Train Epoch: 17 [46080/60000 (76%)]\tLoss: 0.010851\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.026387\n",
            "Train Epoch: 17 [56320/60000 (93%)]\tLoss: 0.025685\n",
            "Train Accuracy: (99%) \n",
            "\n",
            "Average loss: 0.0947, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.009016\n",
            "Train Epoch: 18 [5120/60000 (8%)]\tLoss: 0.003142\n",
            "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.009545\n",
            "Train Epoch: 18 [15360/60000 (25%)]\tLoss: 0.014917\n",
            "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.005140\n",
            "Train Epoch: 18 [25600/60000 (42%)]\tLoss: 0.031934\n",
            "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.017053\n",
            "Train Epoch: 18 [35840/60000 (59%)]\tLoss: 0.023584\n",
            "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.022285\n",
            "Train Epoch: 18 [46080/60000 (76%)]\tLoss: 0.004073\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.022158\n",
            "Train Epoch: 18 [56320/60000 (93%)]\tLoss: 0.009771\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1147, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.031418\n",
            "Train Epoch: 19 [5120/60000 (8%)]\tLoss: 0.023210\n",
            "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.002151\n",
            "Train Epoch: 19 [15360/60000 (25%)]\tLoss: 0.003191\n",
            "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.008576\n",
            "Train Epoch: 19 [25600/60000 (42%)]\tLoss: 0.007733\n",
            "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.010221\n",
            "Train Epoch: 19 [35840/60000 (59%)]\tLoss: 0.027288\n",
            "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.010406\n",
            "Train Epoch: 19 [46080/60000 (76%)]\tLoss: 0.007988\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.017869\n",
            "Train Epoch: 19 [56320/60000 (93%)]\tLoss: 0.010416\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0934, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.005504\n",
            "Train Epoch: 20 [5120/60000 (8%)]\tLoss: 0.011370\n",
            "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.013723\n",
            "Train Epoch: 20 [15360/60000 (25%)]\tLoss: 0.000710\n",
            "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.003544\n",
            "Train Epoch: 20 [25600/60000 (42%)]\tLoss: 0.012191\n",
            "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.000834\n",
            "Train Epoch: 20 [35840/60000 (59%)]\tLoss: 0.017734\n",
            "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.004153\n",
            "Train Epoch: 20 [46080/60000 (76%)]\tLoss: 0.014545\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.016352\n",
            "Train Epoch: 20 [56320/60000 (93%)]\tLoss: 0.006143\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1162, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.023099\n",
            "Train Epoch: 21 [5120/60000 (8%)]\tLoss: 0.021189\n",
            "Train Epoch: 21 [10240/60000 (17%)]\tLoss: 0.019040\n",
            "Train Epoch: 21 [15360/60000 (25%)]\tLoss: 0.001213\n",
            "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 0.015648\n",
            "Train Epoch: 21 [25600/60000 (42%)]\tLoss: 0.002007\n",
            "Train Epoch: 21 [30720/60000 (51%)]\tLoss: 0.007250\n",
            "Train Epoch: 21 [35840/60000 (59%)]\tLoss: 0.013362\n",
            "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 0.008675\n",
            "Train Epoch: 21 [46080/60000 (76%)]\tLoss: 0.022597\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.026065\n",
            "Train Epoch: 21 [56320/60000 (93%)]\tLoss: 0.011225\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1125, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.019954\n",
            "Train Epoch: 22 [5120/60000 (8%)]\tLoss: 0.005323\n",
            "Train Epoch: 22 [10240/60000 (17%)]\tLoss: 0.003546\n",
            "Train Epoch: 22 [15360/60000 (25%)]\tLoss: 0.001225\n",
            "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 0.029357\n",
            "Train Epoch: 22 [25600/60000 (42%)]\tLoss: 0.006142\n",
            "Train Epoch: 22 [30720/60000 (51%)]\tLoss: 0.021591\n",
            "Train Epoch: 22 [35840/60000 (59%)]\tLoss: 0.005196\n",
            "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 0.035989\n",
            "Train Epoch: 22 [46080/60000 (76%)]\tLoss: 0.006406\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.025767\n",
            "Train Epoch: 22 [56320/60000 (93%)]\tLoss: 0.004363\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0987, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.002482\n",
            "Train Epoch: 23 [5120/60000 (8%)]\tLoss: 0.006876\n",
            "Train Epoch: 23 [10240/60000 (17%)]\tLoss: 0.003045\n",
            "Train Epoch: 23 [15360/60000 (25%)]\tLoss: 0.005693\n",
            "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 0.005246\n",
            "Train Epoch: 23 [25600/60000 (42%)]\tLoss: 0.004132\n",
            "Train Epoch: 23 [30720/60000 (51%)]\tLoss: 0.008378\n",
            "Train Epoch: 23 [35840/60000 (59%)]\tLoss: 0.007253\n",
            "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 0.004397\n",
            "Train Epoch: 23 [46080/60000 (76%)]\tLoss: 0.013568\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.006153\n",
            "Train Epoch: 23 [56320/60000 (93%)]\tLoss: 0.013079\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1027, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.010087\n",
            "Train Epoch: 24 [5120/60000 (8%)]\tLoss: 0.001614\n",
            "Train Epoch: 24 [10240/60000 (17%)]\tLoss: 0.004817\n",
            "Train Epoch: 24 [15360/60000 (25%)]\tLoss: 0.001756\n",
            "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 0.010510\n",
            "Train Epoch: 24 [25600/60000 (42%)]\tLoss: 0.011985\n",
            "Train Epoch: 24 [30720/60000 (51%)]\tLoss: 0.005797\n",
            "Train Epoch: 24 [35840/60000 (59%)]\tLoss: 0.009589\n",
            "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 0.005848\n",
            "Train Epoch: 24 [46080/60000 (76%)]\tLoss: 0.053440\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.004502\n",
            "Train Epoch: 24 [56320/60000 (93%)]\tLoss: 0.001135\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0907, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.005439\n",
            "Train Epoch: 25 [5120/60000 (8%)]\tLoss: 0.006024\n",
            "Train Epoch: 25 [10240/60000 (17%)]\tLoss: 0.005455\n",
            "Train Epoch: 25 [15360/60000 (25%)]\tLoss: 0.002051\n",
            "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 0.002232\n",
            "Train Epoch: 25 [25600/60000 (42%)]\tLoss: 0.009630\n",
            "Train Epoch: 25 [30720/60000 (51%)]\tLoss: 0.001759\n",
            "Train Epoch: 25 [35840/60000 (59%)]\tLoss: 0.010926\n",
            "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 0.023335\n",
            "Train Epoch: 25 [46080/60000 (76%)]\tLoss: 0.006283\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.025875\n",
            "Train Epoch: 25 [56320/60000 (93%)]\tLoss: 0.001621\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1025, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.017056\n",
            "Train Epoch: 26 [5120/60000 (8%)]\tLoss: 0.013595\n",
            "Train Epoch: 26 [10240/60000 (17%)]\tLoss: 0.001739\n",
            "Train Epoch: 26 [15360/60000 (25%)]\tLoss: 0.001919\n",
            "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 0.021856\n",
            "Train Epoch: 26 [25600/60000 (42%)]\tLoss: 0.016704\n",
            "Train Epoch: 26 [30720/60000 (51%)]\tLoss: 0.001590\n",
            "Train Epoch: 26 [35840/60000 (59%)]\tLoss: 0.007283\n",
            "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 0.006919\n",
            "Train Epoch: 26 [46080/60000 (76%)]\tLoss: 0.005011\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.005216\n",
            "Train Epoch: 26 [56320/60000 (93%)]\tLoss: 0.001667\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1154, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.012072\n",
            "Train Epoch: 27 [5120/60000 (8%)]\tLoss: 0.000573\n",
            "Train Epoch: 27 [10240/60000 (17%)]\tLoss: 0.000989\n",
            "Train Epoch: 27 [15360/60000 (25%)]\tLoss: 0.002832\n",
            "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 0.005001\n",
            "Train Epoch: 27 [25600/60000 (42%)]\tLoss: 0.007017\n",
            "Train Epoch: 27 [30720/60000 (51%)]\tLoss: 0.020828\n",
            "Train Epoch: 27 [35840/60000 (59%)]\tLoss: 0.044202\n",
            "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 0.008198\n",
            "Train Epoch: 27 [46080/60000 (76%)]\tLoss: 0.005538\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.015890\n",
            "Train Epoch: 27 [56320/60000 (93%)]\tLoss: 0.022881\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0916, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.005132\n",
            "Train Epoch: 28 [5120/60000 (8%)]\tLoss: 0.002099\n",
            "Train Epoch: 28 [10240/60000 (17%)]\tLoss: 0.008921\n",
            "Train Epoch: 28 [15360/60000 (25%)]\tLoss: 0.002390\n",
            "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 0.007396\n",
            "Train Epoch: 28 [25600/60000 (42%)]\tLoss: 0.005873\n",
            "Train Epoch: 28 [30720/60000 (51%)]\tLoss: 0.013134\n",
            "Train Epoch: 28 [35840/60000 (59%)]\tLoss: 0.001919\n",
            "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 0.017491\n",
            "Train Epoch: 28 [46080/60000 (76%)]\tLoss: 0.011564\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.001626\n",
            "Train Epoch: 28 [56320/60000 (93%)]\tLoss: 0.005451\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0967, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.004223\n",
            "Train Epoch: 29 [5120/60000 (8%)]\tLoss: 0.002683\n",
            "Train Epoch: 29 [10240/60000 (17%)]\tLoss: 0.000845\n",
            "Train Epoch: 29 [15360/60000 (25%)]\tLoss: 0.008493\n",
            "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 0.008996\n",
            "Train Epoch: 29 [25600/60000 (42%)]\tLoss: 0.006031\n",
            "Train Epoch: 29 [30720/60000 (51%)]\tLoss: 0.001075\n",
            "Train Epoch: 29 [35840/60000 (59%)]\tLoss: 0.002213\n",
            "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 0.005456\n",
            "Train Epoch: 29 [46080/60000 (76%)]\tLoss: 0.004954\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.010476\n",
            "Train Epoch: 29 [56320/60000 (93%)]\tLoss: 0.000378\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0803, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.003524\n",
            "Train Epoch: 30 [5120/60000 (8%)]\tLoss: 0.001088\n",
            "Train Epoch: 30 [10240/60000 (17%)]\tLoss: 0.006676\n",
            "Train Epoch: 30 [15360/60000 (25%)]\tLoss: 0.006227\n",
            "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 0.004813\n",
            "Train Epoch: 30 [25600/60000 (42%)]\tLoss: 0.002832\n",
            "Train Epoch: 30 [30720/60000 (51%)]\tLoss: 0.000922\n",
            "Train Epoch: 30 [35840/60000 (59%)]\tLoss: 0.000870\n",
            "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 0.001456\n",
            "Train Epoch: 30 [46080/60000 (76%)]\tLoss: 0.003934\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.010040\n",
            "Train Epoch: 30 [56320/60000 (93%)]\tLoss: 0.005596\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0987, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.000663\n",
            "Train Epoch: 31 [5120/60000 (8%)]\tLoss: 0.007062\n",
            "Train Epoch: 31 [10240/60000 (17%)]\tLoss: 0.006384\n",
            "Train Epoch: 31 [15360/60000 (25%)]\tLoss: 0.000773\n",
            "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 0.000390\n",
            "Train Epoch: 31 [25600/60000 (42%)]\tLoss: 0.001543\n",
            "Train Epoch: 31 [30720/60000 (51%)]\tLoss: 0.000289\n",
            "Train Epoch: 31 [35840/60000 (59%)]\tLoss: 0.004731\n",
            "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 0.001484\n",
            "Train Epoch: 31 [46080/60000 (76%)]\tLoss: 0.027127\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.012809\n",
            "Train Epoch: 31 [56320/60000 (93%)]\tLoss: 0.028986\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1364, Accuracy: (97%)\n",
            "\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.039508\n",
            "Train Epoch: 32 [5120/60000 (8%)]\tLoss: 0.007720\n",
            "Train Epoch: 32 [10240/60000 (17%)]\tLoss: 0.009236\n",
            "Train Epoch: 32 [15360/60000 (25%)]\tLoss: 0.010802\n",
            "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 0.008358\n",
            "Train Epoch: 32 [25600/60000 (42%)]\tLoss: 0.000805\n",
            "Train Epoch: 32 [30720/60000 (51%)]\tLoss: 0.004293\n",
            "Train Epoch: 32 [35840/60000 (59%)]\tLoss: 0.001777\n",
            "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 0.003943\n",
            "Train Epoch: 32 [46080/60000 (76%)]\tLoss: 0.007102\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.030737\n",
            "Train Epoch: 32 [56320/60000 (93%)]\tLoss: 0.002566\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1169, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.016231\n",
            "Train Epoch: 33 [5120/60000 (8%)]\tLoss: 0.011309\n",
            "Train Epoch: 33 [10240/60000 (17%)]\tLoss: 0.027225\n",
            "Train Epoch: 33 [15360/60000 (25%)]\tLoss: 0.006317\n",
            "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 0.001370\n",
            "Train Epoch: 33 [25600/60000 (42%)]\tLoss: 0.003563\n",
            "Train Epoch: 33 [30720/60000 (51%)]\tLoss: 0.006877\n",
            "Train Epoch: 33 [35840/60000 (59%)]\tLoss: 0.007239\n",
            "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 0.003478\n",
            "Train Epoch: 33 [46080/60000 (76%)]\tLoss: 0.012475\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.016164\n",
            "Train Epoch: 33 [56320/60000 (93%)]\tLoss: 0.002414\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1118, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.010319\n",
            "Train Epoch: 34 [5120/60000 (8%)]\tLoss: 0.001233\n",
            "Train Epoch: 34 [10240/60000 (17%)]\tLoss: 0.001628\n",
            "Train Epoch: 34 [15360/60000 (25%)]\tLoss: 0.000317\n",
            "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 0.004383\n",
            "Train Epoch: 34 [25600/60000 (42%)]\tLoss: 0.010284\n",
            "Train Epoch: 34 [30720/60000 (51%)]\tLoss: 0.004977\n",
            "Train Epoch: 34 [35840/60000 (59%)]\tLoss: 0.002152\n",
            "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 0.000548\n",
            "Train Epoch: 34 [46080/60000 (76%)]\tLoss: 0.002473\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.000438\n",
            "Train Epoch: 34 [56320/60000 (93%)]\tLoss: 0.002985\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1074, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.003043\n",
            "Train Epoch: 35 [5120/60000 (8%)]\tLoss: 0.000568\n",
            "Train Epoch: 35 [10240/60000 (17%)]\tLoss: 0.005713\n",
            "Train Epoch: 35 [15360/60000 (25%)]\tLoss: 0.001837\n",
            "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 0.002278\n",
            "Train Epoch: 35 [25600/60000 (42%)]\tLoss: 0.003405\n",
            "Train Epoch: 35 [30720/60000 (51%)]\tLoss: 0.001499\n",
            "Train Epoch: 35 [35840/60000 (59%)]\tLoss: 0.008539\n",
            "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 0.003174\n",
            "Train Epoch: 35 [46080/60000 (76%)]\tLoss: 0.005396\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.011234\n",
            "Train Epoch: 35 [56320/60000 (93%)]\tLoss: 0.003415\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1026, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.000885\n",
            "Train Epoch: 36 [5120/60000 (8%)]\tLoss: 0.002837\n",
            "Train Epoch: 36 [10240/60000 (17%)]\tLoss: 0.011274\n",
            "Train Epoch: 36 [15360/60000 (25%)]\tLoss: 0.002645\n",
            "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 0.015806\n",
            "Train Epoch: 36 [25600/60000 (42%)]\tLoss: 0.010721\n",
            "Train Epoch: 36 [30720/60000 (51%)]\tLoss: 0.007474\n",
            "Train Epoch: 36 [35840/60000 (59%)]\tLoss: 0.001371\n",
            "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 0.013864\n",
            "Train Epoch: 36 [46080/60000 (76%)]\tLoss: 0.017071\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.016387\n",
            "Train Epoch: 36 [56320/60000 (93%)]\tLoss: 0.029587\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1142, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.013492\n",
            "Train Epoch: 37 [5120/60000 (8%)]\tLoss: 0.002374\n",
            "Train Epoch: 37 [10240/60000 (17%)]\tLoss: 0.000418\n",
            "Train Epoch: 37 [15360/60000 (25%)]\tLoss: 0.001280\n",
            "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 0.014953\n",
            "Train Epoch: 37 [25600/60000 (42%)]\tLoss: 0.035494\n",
            "Train Epoch: 37 [30720/60000 (51%)]\tLoss: 0.005840\n",
            "Train Epoch: 37 [35840/60000 (59%)]\tLoss: 0.007220\n",
            "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 0.019475\n",
            "Train Epoch: 37 [46080/60000 (76%)]\tLoss: 0.016614\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.006034\n",
            "Train Epoch: 37 [56320/60000 (93%)]\tLoss: 0.006087\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0933, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.003144\n",
            "Train Epoch: 38 [5120/60000 (8%)]\tLoss: 0.002111\n",
            "Train Epoch: 38 [10240/60000 (17%)]\tLoss: 0.001338\n",
            "Train Epoch: 38 [15360/60000 (25%)]\tLoss: 0.014856\n",
            "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 0.001603\n",
            "Train Epoch: 38 [25600/60000 (42%)]\tLoss: 0.001968\n",
            "Train Epoch: 38 [30720/60000 (51%)]\tLoss: 0.008355\n",
            "Train Epoch: 38 [35840/60000 (59%)]\tLoss: 0.013499\n",
            "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 0.037364\n",
            "Train Epoch: 38 [46080/60000 (76%)]\tLoss: 0.015618\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.026105\n",
            "Train Epoch: 38 [56320/60000 (93%)]\tLoss: 0.002029\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0970, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.014076\n",
            "Train Epoch: 39 [5120/60000 (8%)]\tLoss: 0.007124\n",
            "Train Epoch: 39 [10240/60000 (17%)]\tLoss: 0.000496\n",
            "Train Epoch: 39 [15360/60000 (25%)]\tLoss: 0.001277\n",
            "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 0.003618\n",
            "Train Epoch: 39 [25600/60000 (42%)]\tLoss: 0.000122\n",
            "Train Epoch: 39 [30720/60000 (51%)]\tLoss: 0.000289\n",
            "Train Epoch: 39 [35840/60000 (59%)]\tLoss: 0.001858\n",
            "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 0.000092\n",
            "Train Epoch: 39 [46080/60000 (76%)]\tLoss: 0.001780\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.006393\n",
            "Train Epoch: 39 [56320/60000 (93%)]\tLoss: 0.003891\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0903, Accuracy: (99%)\n",
            "\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.000204\n",
            "Train Epoch: 40 [5120/60000 (8%)]\tLoss: 0.000167\n",
            "Train Epoch: 40 [10240/60000 (17%)]\tLoss: 0.000458\n",
            "Train Epoch: 40 [15360/60000 (25%)]\tLoss: 0.000662\n",
            "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 0.002994\n",
            "Train Epoch: 40 [25600/60000 (42%)]\tLoss: 0.004987\n",
            "Train Epoch: 40 [30720/60000 (51%)]\tLoss: 0.011832\n",
            "Train Epoch: 40 [35840/60000 (59%)]\tLoss: 0.004729\n",
            "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 0.003023\n",
            "Train Epoch: 40 [46080/60000 (76%)]\tLoss: 0.001086\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.007857\n",
            "Train Epoch: 40 [56320/60000 (93%)]\tLoss: 0.002088\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1160, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.010233\n",
            "Train Epoch: 41 [5120/60000 (8%)]\tLoss: 0.008225\n",
            "Train Epoch: 41 [10240/60000 (17%)]\tLoss: 0.009754\n",
            "Train Epoch: 41 [15360/60000 (25%)]\tLoss: 0.006729\n",
            "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 0.013964\n",
            "Train Epoch: 41 [25600/60000 (42%)]\tLoss: 0.008059\n",
            "Train Epoch: 41 [30720/60000 (51%)]\tLoss: 0.003632\n",
            "Train Epoch: 41 [35840/60000 (59%)]\tLoss: 0.000476\n",
            "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 0.000778\n",
            "Train Epoch: 41 [46080/60000 (76%)]\tLoss: 0.000326\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.008360\n",
            "Train Epoch: 41 [56320/60000 (93%)]\tLoss: 0.000168\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1111, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.000459\n",
            "Train Epoch: 42 [5120/60000 (8%)]\tLoss: 0.004049\n",
            "Train Epoch: 42 [10240/60000 (17%)]\tLoss: 0.000507\n",
            "Train Epoch: 42 [15360/60000 (25%)]\tLoss: 0.001040\n",
            "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 0.008169\n",
            "Train Epoch: 42 [25600/60000 (42%)]\tLoss: 0.010669\n",
            "Train Epoch: 42 [30720/60000 (51%)]\tLoss: 0.024268\n",
            "Train Epoch: 42 [35840/60000 (59%)]\tLoss: 0.014764\n",
            "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 0.010949\n",
            "Train Epoch: 42 [46080/60000 (76%)]\tLoss: 0.020709\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.000271\n",
            "Train Epoch: 42 [56320/60000 (93%)]\tLoss: 0.005024\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1024, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.006048\n",
            "Train Epoch: 43 [5120/60000 (8%)]\tLoss: 0.029696\n",
            "Train Epoch: 43 [10240/60000 (17%)]\tLoss: 0.003933\n",
            "Train Epoch: 43 [15360/60000 (25%)]\tLoss: 0.016379\n",
            "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 0.001299\n",
            "Train Epoch: 43 [25600/60000 (42%)]\tLoss: 0.008390\n",
            "Train Epoch: 43 [30720/60000 (51%)]\tLoss: 0.005527\n",
            "Train Epoch: 43 [35840/60000 (59%)]\tLoss: 0.021226\n",
            "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 0.002945\n",
            "Train Epoch: 43 [46080/60000 (76%)]\tLoss: 0.005125\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.003849\n",
            "Train Epoch: 43 [56320/60000 (93%)]\tLoss: 0.000413\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1195, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.001482\n",
            "Train Epoch: 44 [5120/60000 (8%)]\tLoss: 0.024710\n",
            "Train Epoch: 44 [10240/60000 (17%)]\tLoss: 0.000257\n",
            "Train Epoch: 44 [15360/60000 (25%)]\tLoss: 0.030073\n",
            "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 0.000802\n",
            "Train Epoch: 44 [25600/60000 (42%)]\tLoss: 0.001216\n",
            "Train Epoch: 44 [30720/60000 (51%)]\tLoss: 0.002121\n",
            "Train Epoch: 44 [35840/60000 (59%)]\tLoss: 0.021722\n",
            "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 0.001849\n",
            "Train Epoch: 44 [46080/60000 (76%)]\tLoss: 0.003585\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.007084\n",
            "Train Epoch: 44 [56320/60000 (93%)]\tLoss: 0.001977\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0852, Accuracy: (99%)\n",
            "\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.002899\n",
            "Train Epoch: 45 [5120/60000 (8%)]\tLoss: 0.022004\n",
            "Train Epoch: 45 [10240/60000 (17%)]\tLoss: 0.003299\n",
            "Train Epoch: 45 [15360/60000 (25%)]\tLoss: 0.001944\n",
            "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 0.004695\n",
            "Train Epoch: 45 [25600/60000 (42%)]\tLoss: 0.000109\n",
            "Train Epoch: 45 [30720/60000 (51%)]\tLoss: 0.004569\n",
            "Train Epoch: 45 [35840/60000 (59%)]\tLoss: 0.008227\n",
            "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 0.009277\n",
            "Train Epoch: 45 [46080/60000 (76%)]\tLoss: 0.000526\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.000080\n",
            "Train Epoch: 45 [56320/60000 (93%)]\tLoss: 0.006648\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1114, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.000805\n",
            "Train Epoch: 46 [5120/60000 (8%)]\tLoss: 0.000284\n",
            "Train Epoch: 46 [10240/60000 (17%)]\tLoss: 0.000999\n",
            "Train Epoch: 46 [15360/60000 (25%)]\tLoss: 0.001615\n",
            "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 0.003913\n",
            "Train Epoch: 46 [25600/60000 (42%)]\tLoss: 0.000223\n",
            "Train Epoch: 46 [30720/60000 (51%)]\tLoss: 0.009763\n",
            "Train Epoch: 46 [35840/60000 (59%)]\tLoss: 0.000170\n",
            "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 0.011870\n",
            "Train Epoch: 46 [46080/60000 (76%)]\tLoss: 0.007946\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.001178\n",
            "Train Epoch: 46 [56320/60000 (93%)]\tLoss: 0.010592\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.0998, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.000758\n",
            "Train Epoch: 47 [5120/60000 (8%)]\tLoss: 0.003987\n",
            "Train Epoch: 47 [10240/60000 (17%)]\tLoss: 0.001485\n",
            "Train Epoch: 47 [15360/60000 (25%)]\tLoss: 0.000921\n",
            "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 0.004384\n",
            "Train Epoch: 47 [25600/60000 (42%)]\tLoss: 0.000891\n",
            "Train Epoch: 47 [30720/60000 (51%)]\tLoss: 0.027552\n",
            "Train Epoch: 47 [35840/60000 (59%)]\tLoss: 0.000555\n",
            "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 0.025989\n",
            "Train Epoch: 47 [46080/60000 (76%)]\tLoss: 0.017713\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.007905\n",
            "Train Epoch: 47 [56320/60000 (93%)]\tLoss: 0.008194\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1154, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.029261\n",
            "Train Epoch: 48 [5120/60000 (8%)]\tLoss: 0.005369\n",
            "Train Epoch: 48 [10240/60000 (17%)]\tLoss: 0.012209\n",
            "Train Epoch: 48 [15360/60000 (25%)]\tLoss: 0.001140\n",
            "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 0.014547\n",
            "Train Epoch: 48 [25600/60000 (42%)]\tLoss: 0.001697\n",
            "Train Epoch: 48 [30720/60000 (51%)]\tLoss: 0.000481\n",
            "Train Epoch: 48 [35840/60000 (59%)]\tLoss: 0.011297\n",
            "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 0.007041\n",
            "Train Epoch: 48 [46080/60000 (76%)]\tLoss: 0.011940\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.001764\n",
            "Train Epoch: 48 [56320/60000 (93%)]\tLoss: 0.000241\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1208, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.000703\n",
            "Train Epoch: 49 [5120/60000 (8%)]\tLoss: 0.012523\n",
            "Train Epoch: 49 [10240/60000 (17%)]\tLoss: 0.002422\n",
            "Train Epoch: 49 [15360/60000 (25%)]\tLoss: 0.001740\n",
            "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 0.000310\n",
            "Train Epoch: 49 [25600/60000 (42%)]\tLoss: 0.002701\n",
            "Train Epoch: 49 [30720/60000 (51%)]\tLoss: 0.001767\n",
            "Train Epoch: 49 [35840/60000 (59%)]\tLoss: 0.001321\n",
            "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 0.001687\n",
            "Train Epoch: 49 [46080/60000 (76%)]\tLoss: 0.000080\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.003206\n",
            "Train Epoch: 49 [56320/60000 (93%)]\tLoss: 0.018324\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1135, Accuracy: (98%)\n",
            "\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.003636\n",
            "Train Epoch: 50 [5120/60000 (8%)]\tLoss: 0.000448\n",
            "Train Epoch: 50 [10240/60000 (17%)]\tLoss: 0.001114\n",
            "Train Epoch: 50 [15360/60000 (25%)]\tLoss: 0.005783\n",
            "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 0.005845\n",
            "Train Epoch: 50 [25600/60000 (42%)]\tLoss: 0.002453\n",
            "Train Epoch: 50 [30720/60000 (51%)]\tLoss: 0.016868\n",
            "Train Epoch: 50 [35840/60000 (59%)]\tLoss: 0.000825\n",
            "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 0.004151\n",
            "Train Epoch: 50 [46080/60000 (76%)]\tLoss: 0.000441\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.001783\n",
            "Train Epoch: 50 [56320/60000 (93%)]\tLoss: 0.013036\n",
            "Train Accuracy: (100%) \n",
            "\n",
            "Average loss: 0.1120, Accuracy: (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(args_1.model_a)\n",
        "model_a.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "8NIt-GXKv3kg",
        "outputId": "465717ba-3072-44ff-da34-325497f76e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Args' object has no attribute 'model_a'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-fb48dc07c0f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Args' object has no attribute 'model_a'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torch torchvision\n",
        "\n",
        "torch.save(model_a.state_dict(), f\"/content/drive/My Drive/mnist_mlp_{str(args_1.seed)}.pt\")\n",
        "\n",
        "# checkpoint = torch.load(f\"/content/drive/My Drive/{args_1.model}\")\n",
        "\n",
        "torch.save(model_b.state_dict(), f\"/content/drive/My Drive/mnist_mlp_{str(args_2.seed)}.pt\")\n",
        "\n",
        "# checkpoint = torch.load(f\"/content/drive/My Drive/{args_2.model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUaoO_eDFABi",
        "outputId": "bbbefdc3-ca10-4772-f932-f73281236606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model_a_saved = MLP()\n",
        "model_b_saved = MLP()\n",
        "model_a_saved.load_state_dict(torch.load(\"/content/drive/My Drive/mnist_mlp_1.pt\"))\n",
        "model_b_saved.load_state_dict(torch.load(\"/content/drive/My Drive/mnist_mlp_2.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-0yPMCFz61_",
        "outputId": "d61c3aef-9494-4e03-a1bb-d6d646b62077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from utils.weight_matching import mlp_permutation_spec, weight_matching, apply_permutation\n",
        "# from utils.utils import flatten_params, lerp\n",
        "# from utils.plot import plot_interp_acc\n",
        "import argparse\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "# from utils.training import test\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--model_a\", type=str, required=True)\n",
        "# parser.add_argument(\"--model_b\", type=str, required=True)\n",
        "# parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed\")\n",
        "# args = parser.parse_args()\n",
        "\n",
        "\n",
        "permutation_spec = mlp_permutation_spec(4)\n",
        "final_permutation = weight_matching(permutation_spec,\n",
        "                                    flatten_params(model_a_saved), flatten_params(model_b_saved))\n",
        "\n",
        "\n",
        "updated_params = apply_permutation(permutation_spec, final_permutation, flatten_params(model_b_saved))\n",
        "\n",
        "\n",
        "# test against mnist\n",
        "transform=transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.1307,), (0.3081,))\n",
        "  ])\n",
        "test_kwargs = {'batch_size': 5000}\n",
        "train_kwargs = {'batch_size': 5000}\n",
        "dataset = datasets.MNIST('../data', train=False, download=True,\n",
        "                  transform=transform)\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                  transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset, **test_kwargs)\n",
        "lambdas = torch.linspace(0, 1, steps=25)\n",
        "\n",
        "test_acc_interp_clever = []\n",
        "test_acc_interp_naive = []\n",
        "train_acc_interp_clever = []\n",
        "train_acc_interp_naive = []\n",
        "# naive\n",
        "model_b_saved.load_state_dict(torch.load(\"/content/drive/My Drive/mnist_mlp_2.pt\"))\n",
        "model_a_dict = copy.deepcopy(model_a_saved.state_dict())\n",
        "model_b_dict = copy.deepcopy(model_b_saved.state_dict())\n",
        "for lam in tqdm(lambdas):\n",
        "  naive_p = lerp(lam, model_a_dict, model_b_dict)\n",
        "  model_b_saved.load_state_dict(naive_p)\n",
        "  test_loss, acc = test(model_b_saved.cuda(), 'cuda', test_loader)\n",
        "  test_acc_interp_naive.append(acc)\n",
        "  train_loss, acc = test(model_b_saved.cuda(), 'cuda', train_loader)\n",
        "  train_acc_interp_naive.append(acc)\n",
        "\n",
        "# smart\n",
        "model_b_saved.load_state_dict(updated_params)\n",
        "model_b_saved.cuda()\n",
        "model_a_saved.cuda()\n",
        "model_a_dict = copy.deepcopy(model_a_saved.state_dict())\n",
        "model_b_dict = copy.deepcopy(model_b_saved.state_dict())\n",
        "for lam in tqdm(lambdas):\n",
        "  naive_p = lerp(lam, model_a_dict, model_b_dict)\n",
        "  model_b_saved.load_state_dict(naive_p)\n",
        "  test_loss, acc = test(model_b_saved.cuda(), 'cuda', test_loader)\n",
        "  test_acc_interp_clever.append(acc)\n",
        "  train_loss, acc = test(model_b_saved.cuda(), 'cuda', train_loader)\n",
        "  train_acc_interp_clever.append(acc)\n",
        "\n",
        "fig = plot_interp_acc(lambdas, train_acc_interp_naive, test_acc_interp_naive,\n",
        "                train_acc_interp_clever, test_acc_interp_clever)\n",
        "plt.savefig(f\"mnist_mlp_weight_matching_interp_accuracy_epoch.png\", dpi=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wdXH6Rzt0MXv",
        "outputId": "59b4cd48-fdf2-4f25-8476-2388ae670148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/P_0: 499.32867431640625\n",
            "0/P_2: 89.97660827636719\n",
            "0/P_3: 21.238574981689453\n",
            "0/P_1: 213.55636596679688\n",
            "1/P_3: 0.0\n",
            "1/P_1: 0.0\n",
            "1/P_2: 109.57341766357422\n",
            "1/P_0: 13.1912841796875\n",
            "2/P_1: 36.513885498046875\n",
            "2/P_2: 16.9544677734375\n",
            "2/P_0: 3.15313720703125\n",
            "2/P_3: 34.439029693603516\n",
            "3/P_3: 0.0\n",
            "3/P_0: 0.0\n",
            "3/P_1: 4.53521728515625\n",
            "3/P_2: 8.58795166015625\n",
            "4/P_2: 0.0\n",
            "4/P_0: 0.43841552734375\n",
            "4/P_3: 1.7426223754882812\n",
            "4/P_1: 1.507080078125\n",
            "5/P_1: 0.0\n",
            "5/P_2: 2.103424072265625\n",
            "5/P_3: 0.5877609252929688\n",
            "5/P_0: 0.36474609375\n",
            "6/P_2: 0.156402587890625\n",
            "6/P_1: 0.629150390625\n",
            "6/P_0: 0.096435546875\n",
            "6/P_3: 0.07738494873046875\n",
            "7/P_0: 0.0\n",
            "7/P_1: 0.2720947265625\n",
            "7/P_2: 0.602752685546875\n",
            "7/P_3: 0.42014312744140625\n",
            "8/P_2: 0.244964599609375\n",
            "8/P_1: 0.10687255859375\n",
            "8/P_0: 0.06011962890625\n",
            "8/P_3: 0.08878326416015625\n",
            "9/P_2: 0.240142822265625\n",
            "9/P_3: 0.018890380859375\n",
            "9/P_0: 0.0\n",
            "9/P_1: 0.037353515625\n",
            "10/P_1: 0.0\n",
            "10/P_0: 0.0\n",
            "10/P_2: 0.076385498046875\n",
            "10/P_3: 0.0220794677734375\n",
            "11/P_2: 0.030914306640625\n",
            "11/P_3: 0.0267791748046875\n",
            "11/P_0: 0.0\n",
            "11/P_1: 0.01629638671875\n",
            "12/P_0: 0.0\n",
            "12/P_1: 0.0\n",
            "12/P_3: 0.0\n",
            "12/P_2: 0.016754150390625\n",
            "13/P_2: 0.0\n",
            "13/P_0: 0.0\n",
            "13/P_1: 0.0140380859375\n",
            "13/P_3: 0.02276611328125\n",
            "14/P_3: 0.0\n",
            "14/P_2: 0.10614013671875\n",
            "14/P_0: 0.0\n",
            "14/P_1: 0.0\n",
            "15/P_2: 0.0\n",
            "15/P_1: 0.0\n",
            "15/P_3: 0.0378570556640625\n",
            "15/P_0: 0.0\n",
            "16/P_3: 0.0\n",
            "16/P_1: 0.0\n",
            "16/P_0: 0.0\n",
            "16/P_2: 0.007720947265625\n",
            "17/P_0: 0.0\n",
            "17/P_3: 0.00959014892578125\n",
            "17/P_1: 0.0\n",
            "17/P_2: 0.0635986328125\n",
            "18/P_2: 0.0\n",
            "18/P_3: 0.04219818115234375\n",
            "18/P_0: 0.0\n",
            "18/P_1: 0.01348876953125\n",
            "19/P_3: 0.0\n",
            "19/P_2: 0.0303955078125\n",
            "19/P_0: 0.00531005859375\n",
            "19/P_1: 0.00341796875\n",
            "20/P_3: 0.01194000244140625\n",
            "20/P_1: 0.0\n",
            "20/P_2: 0.00140380859375\n",
            "20/P_0: 0.0\n",
            "21/P_2: 0.0\n",
            "21/P_0: 0.0\n",
            "21/P_1: 0.0\n",
            "21/P_3: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/25 [00:00<?, ?it/s]<ipython-input-5-3775b565d8d0>:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return nn.functional.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.1120, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 1/25 [00:16<06:33, 16.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0058, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0880, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 2/25 [00:32<06:18, 16.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0060, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0739, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 3/25 [00:49<06:06, 16.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0094, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0743, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 4/25 [01:07<06:00, 17.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0218, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.1064, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 5/25 [01:23<05:32, 16.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0619, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.2146, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 6/25 [01:38<05:08, 16.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.1758, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.4614, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 7/25 [01:54<04:48, 16.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.4267, Accuracy: (99%)\n",
            "\n",
            "\n",
            "Average loss: 0.8499, Accuracy: (97%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 8/25 [02:10<04:30, 15.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.8202, Accuracy: (98%)\n",
            "\n",
            "\n",
            "Average loss: 1.2847, Accuracy: (94%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 9/25 [02:25<04:12, 15.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 1.2627, Accuracy: (96%)\n",
            "\n",
            "\n",
            "Average loss: 1.6556, Accuracy: (86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 10/25 [02:41<03:56, 15.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 1.6420, Accuracy: (87%)\n",
            "\n",
            "\n",
            "Average loss: 1.9154, Accuracy: (57%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 11/25 [02:57<03:40, 15.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 1.9084, Accuracy: (59%)\n",
            "\n",
            "\n",
            "Average loss: 2.0671, Accuracy: (31%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 12/25 [03:12<03:24, 15.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 2.0641, Accuracy: (31%)\n",
            "\n",
            "\n",
            "Average loss: 2.1180, Accuracy: (22%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 13/25 [03:28<03:08, 15.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 2.1172, Accuracy: (23%)\n",
            "\n",
            "\n",
            "Average loss: 2.0609, Accuracy: (34%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 14/25 [03:45<02:57, 16.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 2.0611, Accuracy: (34%)\n",
            "\n",
            "\n",
            "Average loss: 1.8890, Accuracy: (57%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 15/25 [04:01<02:39, 15.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 1.8885, Accuracy: (57%)\n",
            "\n",
            "\n",
            "Average loss: 1.5971, Accuracy: (80%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 16/25 [04:16<02:23, 15.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 1.5949, Accuracy: (81%)\n",
            "\n",
            "\n",
            "Average loss: 1.1953, Accuracy: (91%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 17/25 [04:32<02:06, 15.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 1.1892, Accuracy: (93%)\n",
            "\n",
            "\n",
            "Average loss: 0.7514, Accuracy: (96%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 18/25 [04:47<01:49, 15.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.7375, Accuracy: (97%)\n",
            "\n",
            "\n",
            "Average loss: 0.3845, Accuracy: (97%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 19/25 [05:03<01:34, 15.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.3591, Accuracy: (99%)\n",
            "\n",
            "\n",
            "Average loss: 0.1735, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 20/25 [05:20<01:20, 16.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.1371, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0886, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 21/25 [05:36<01:04, 16.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0447, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0642, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 22/25 [05:52<00:47, 15.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0140, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0637, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 23/25 [06:07<00:31, 15.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0050, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0745, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 24/25 [06:23<00:15, 15.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0026, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0935, Accuracy: (99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [06:40<00:00, 16.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0022, Accuracy: (100%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/25 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.1120, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 1/25 [00:15<06:14, 15.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0058, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.1024, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 2/25 [00:30<05:56, 15.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0049, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0941, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 3/25 [00:46<05:42, 15.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0045, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0873, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 4/25 [01:02<05:28, 15.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0045, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0819, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 5/25 [01:18<05:14, 15.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0049, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0777, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 6/25 [01:34<04:58, 15.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0055, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0742, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 7/25 [01:50<04:44, 15.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0064, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0717, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 8/25 [02:05<04:28, 15.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0073, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0699, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 9/25 [02:21<04:14, 15.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0083, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0687, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 10/25 [02:38<03:59, 16.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0092, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0678, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 11/25 [02:56<03:55, 16.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0099, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0668, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 12/25 [03:12<03:35, 16.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0102, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0660, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 13/25 [03:28<03:15, 16.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0102, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0654, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 14/25 [03:44<02:58, 16.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0097, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0650, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 15/25 [04:00<02:41, 16.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0088, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0651, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 16/25 [04:16<02:24, 16.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0077, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0656, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 17/25 [04:32<02:08, 16.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0064, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0664, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 18/25 [04:48<01:52, 16.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0052, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0678, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 19/25 [05:04<01:36, 16.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0041, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0699, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 20/25 [05:19<01:19, 15.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0033, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0730, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 21/25 [05:35<01:02, 15.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0027, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0767, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 22/25 [05:53<00:49, 16.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0023, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0812, Accuracy: (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 23/25 [06:08<00:32, 16.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0022, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0866, Accuracy: (99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 24/25 [06:24<00:15, 15.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0021, Accuracy: (100%)\n",
            "\n",
            "\n",
            "Average loss: 0.0936, Accuracy: (99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [06:39<00:00, 15.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average loss: 0.0022, Accuracy: (100%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}